<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jun Zhu Li">
<meta name="dcterms.date" content="2025-06-06">

<title>Machine Learning – Jun’s Website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9f0887447d3a639491f60c4af03832d.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-dc376453fc93729a66e4175d27e645db.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jun’s Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:jzhuuli@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/junzhuli"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a.-k-means" id="toc-a.-k-means" class="nav-link active" data-scroll-target="#a.-k-means">1a. K-Means</a></li>
  <li><a href="#clustering-animation" id="toc-clustering-animation" class="nav-link" data-scroll-target="#clustering-animation">Clustering Animation</a></li>
  <li><a href="#a.-k-nearest-neighbors" id="toc-a.-k-nearest-neighbors" class="nav-link" data-scroll-target="#a.-k-nearest-neighbors">2a. K Nearest Neighbors</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Machine Learning</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jun Zhu Li </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 6, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- _todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._ -->
<section id="a.-k-means" class="level2">
<h2 class="anchored" data-anchor-id="a.-k-means">1a. K-Means</h2>
<!-- _todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_

_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._ -->
<!-- ```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

penguins = pd.read_csv("palmer_penguins.csv")  # Adjust path if needed

X = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values

# ----------- K-Means Function -----------

def initialize_centroids(X, k):
    indices = np.random.choice(len(X), size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def k_means_with_tracking(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    history = []
    
    for iteration in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        history.append((centroids.copy(), labels.copy()))
        
        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol):
            break
        centroids = new_centroids
        
    return labels, centroids, history

# ----------- Run Custom K-Means -----------

k = 3
labels_custom, centroids_custom, history = k_means_with_tracking(X, k)

# ----------------- Plots ------------------

for step, (centroids_step, labels_step) in enumerate(history):
    plt.figure(figsize=(6, 5))
    for i in range(k):
        plt.scatter(X[labels_step == i][:, 0], X[labels_step == i][:, 1], label=f'Cluster {i+1}')
    plt.scatter(centroids_step[:, 0], centroids_step[:, 1], c='black', s=150, marker='X', label='Centroids')
    plt.title(f'K-Means Iteration {step + 1}')
    plt.xlabel('Bill Length (mm)')
    plt.ylabel('Flipper Length (mm)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

``` -->
<p>To explore the fundamentals of unsupervised learning, I implemented the K-Means clustering algorithm from scratch and applied it to the Palmer Penguins dataset. I specifically focused on two continuous and biologically meaningful features: bill length (in millimeters) and flipper length (in millimeters). These variables provide a useful basis for clustering because they exhibit noticeable differences across species and lend themselves well to two-dimensional visualization. My implementation of K-Means followed the classic structure: initializing random centroids, assigning each data point to the nearest centroid using Euclidean distance, updating centroids as the mean of their assigned points, and repeating this process until convergence. To enhance interpretability, I visualized the algorithm’s progress at each iteration, showing how clusters gradually formed and stabilized as the centroids moved and the point assignments changed. These visualizations provided a tangible understanding of how K-Means optimizes the placement of centroids and the grouping of data.</p>
<p>After confirming the correctness of my custom algorithm through visual inspection, I compared my results to Python’s built-in KMeans function from the sklearn.cluster module. The outputs from both implementations were nearly identical, reinforcing that my manual version performed as expected. With clusters that corresponded visually to three distinct groupings—likely reflecting the known Adelie, Gentoo, and Chinstrap species—it was clear that K-Means could discover structure in the data without supervision. Following this, I moved on to evaluate the optimal number of clusters using two widely accepted metrics: the within-cluster sum of squares (WCSS), which assesses compactness of clusters, and the silhouette score, which measures how well-separated the clusters are. I used my custom K-Means algorithm to generate cluster labels for each K value from 2 through 7, and then computed WCSS manually while using the built-in silhouette_score function from sklearn.metrics.</p>
<p>The results of this evaluation revealed several insights. WCSS decreased consistently as K increased, as expected, but the rate of decrease sharply diminished after K=3—producing the classic “elbow” shape that suggests an optimal K. Silhouette scores peaked at K=2, indicating the tightest and most well-separated clusters at that level, but remained relatively high at K=3. Beyond K=3, the silhouette scores declined more rapidly, particularly at K=6, suggesting that larger values of K may fragment the data too much and reduce cluster quality. These metrics jointly suggest that K=2 is statistically optimal, but K=3 offers a balance between model performance and interpretability. Since the dataset is known to contain three penguin species, K=3 is the most reasonable and domain-justified choice.</p>
<p>Overall, this analysis confirmed the power and intuition behind K-Means clustering. Writing the algorithm from scratch provided a deeper understanding of the mechanics behind clustering, while visualizing each iteration helped demystify the convergence process. The comparison with the built-in implementation validated my work, and the metric-based evaluation helped inform the most appropriate number of clusters. The combination of quantitative analysis and domain knowledge led to the conclusion that K=3 is the most interpretable and statistically justifiable clustering solution for the Palmer Penguins dataset.</p>
<div id="99c42ebb" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> imageio</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> pd.read_csv(<span class="st">"palmer_penguins.csv"</span>)  <span class="co"># Adjust path if needed</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">'bill_length_mm'</span>, <span class="st">'flipper_length_mm'</span>]].dropna().values</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- K-Means Function -----------</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_centroids(X, k):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), size<span class="op">=</span>k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[indices]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assign_clusters(X, centroids):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    distances <span class="op">=</span> np.linalg.norm(X[:, np.newaxis] <span class="op">-</span> centroids, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_centroids(X, labels, k):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([X[labels <span class="op">==</span> i].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_means_with_tracking(X, k, max_iters<span class="op">=</span><span class="dv">100</span>, tol<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> initialize_centroids(X, k)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> []</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> assign_clusters(X, centroids)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        new_centroids <span class="op">=</span> update_centroids(X, labels, k)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        history.append((centroids.copy(), labels.copy()))</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(np.linalg.norm(new_centroids <span class="op">-</span> centroids, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">&lt;</span> tol):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> new_centroids</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels, centroids, history</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- Run Custom K-Means -----------</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>labels_custom, centroids_custom, history <span class="op">=</span> k_means_with_tracking(X, k)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- Create Animated GIF -----------</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">"kmeans_frames"</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>filenames <span class="op">=</span> []</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (centroids_step, labels_step) <span class="kw">in</span> <span class="bu">enumerate</span>(history):</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[labels_step <span class="op">==</span> i][:, <span class="dv">0</span>], X[labels_step <span class="op">==</span> i][:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    ax.scatter(centroids_step[:, <span class="dv">0</span>], centroids_step[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">150</span>, marker<span class="op">=</span><span class="st">'X'</span>, label<span class="op">=</span><span class="st">'Centroids'</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'K-Means Iteration </span><span class="sc">{</span>step <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Bill Length (mm)'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Flipper Length (mm)'</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    filename <span class="op">=</span> <span class="ss">f'kmeans_frames/frame_</span><span class="sc">{</span>step<span class="sc">:02d}</span><span class="ss">.png'</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    plt.savefig(filename)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    filenames.append(filename)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>gif_path <span class="op">=</span> <span class="st">'kmeans_clustering.gif'</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> imageio.get_writer(gif_path, mode<span class="op">=</span><span class="st">'I'</span>, duration<span class="op">=</span><span class="fl">0.8</span>, loop<span class="op">=</span><span class="dv">0</span>) <span class="im">as</span> writer:</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> filename <span class="kw">in</span> filenames:</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> imageio.imread(filename)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        writer.append_data(image)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> filenames:</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    os.remove(filename)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Final dummy assignment to suppress output (if needed)</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/69/xm9kgxts537cl3n81zl1z2mw0000gn/T/ipykernel_37298/3395852467.py:69: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.
  image = imageio.imread(filename)</code></pre>
</div>
</div>
</section>
<section id="clustering-animation" class="level2">
<h2 class="anchored" data-anchor-id="clustering-animation">Clustering Animation</h2>
<p>Here’s how the K-Means algorithm iteratively converges:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="kmeans_clustering.gif" class="img-fluid figure-img"></p>
<figcaption>K-Means Clustering Animation</figcaption>
</figure>
</div>
<div id="fa92d0f2" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>K_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">8</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>wcss_values <span class="op">=</span> []</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>silhouette_scores <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> K_range:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    labels, centroids, _ <span class="op">=</span> k_means_with_tracking(X, k)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute WCSS manually</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    wcss <span class="op">=</span> <span class="bu">sum</span>(np.<span class="bu">sum</span>((X[labels <span class="op">==</span> i] <span class="op">-</span> centroids[i])<span class="op">**</span><span class="dv">2</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    wcss_values.append(wcss)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute silhouette score using sklearn</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        sil_score <span class="op">=</span> silhouette_score(X, labels)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append(sil_score)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append(np.nan)  <span class="co"># Not defined for k=1</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot WCSS and Silhouette Score</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># WCSS plot</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(K_range, wcss_values, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Within-Cluster Sum of Squares (WCSS)'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Number of Clusters (K)'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'WCSS'</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Silhouette Score plot</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(K_range, silhouette_scores, marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Silhouette Score'</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Clusters (K)'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Silhouette Score'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<!-- ## 1b. Latent-Class MNL

_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._

_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._

_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._

_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._

_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._ -->
</section>
<section id="a.-k-nearest-neighbors" class="level2">
<h2 class="anchored" data-anchor-id="a.-k-nearest-neighbors">2a. K Nearest Neighbors</h2>
<!-- _todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._ -->
<!-- ```{r}
# gen data -----
set.seed(42)
n <- 100
x1 <- runif(n, -3, 3)
x2 <- runif(n, -3, 3)
x <- cbind(x1, x2)

# define a wiggly boundary
boundary <- sin(4*x1) + x1
y <- ifelse(x2 > boundary, 1, 0) |> as.factor()
dat <- data.frame(x1 = x1, x2 = x2, y = y)
``` -->
<!-- $\beta_\text{price}$ -->
<p>I implemented and analyzed the K-Nearest Neighbors (KNN) algorithm using a fully synthetic dataset designed to challenge classifiers with a nonlinear decision boundary. The dataset was generated using two features, x1 and x2, each sampled uniformly between -3 and 3. Labels were assigned based on whether a point lay above or below a wiggly curve defined by the function <span class="math inline">\(\text{x2 = sin(4x1) + x1}\)</span>, creating a binary classification task. The training data was visualized with the decision boundary clearly plotted, which provided an intuitive understanding of how difficult the classification would be for more rigid models. This plot showed red and blue classes (1 and 0) separated by a sinusoidal-like boundary, hinting at the complexity of the decision space.</p>
<p>To validate the generalization performance of the algorithm, I created a separate test set using the same process but a different random seed to ensure independence. With the data prepared, I implemented the KNN algorithm from scratch. The custom version computed Euclidean distances between each test point and all training points, identified the k closest neighbors, and made a prediction based on majority voting among their labels. This approach provided transparency into how each prediction was made, and reinforced my understanding of distance-based learning.</p>
<p>To evaluate the performance of my implementation, I compared it directly with the built-in KNeighborsClassifier from the scikit-learn library. For both versions of KNN, I iterated over values of k from 1 to 30, computing the percentage of correctly classified test points at each step. The resulting plot showed that both the custom and built-in versions tracked nearly identical accuracy curves, validating the correctness of my implementation. The highest accuracy occurred at very low k values, specifically around k = 1 or k = 2, and declined gradually as k increased. This is consistent with the nature of the problem — a highly nonlinear boundary requires a low-bias model like KNN with small k to closely fit the data’s shape.</p>
<p>The key insight from this exercise was the balance between flexibility and generalization in KNN. While a very small k can lead to overfitting in noisier datasets, in this case, the complexity of the true boundary made low k optimal. Another takeaway was the robustness and simplicity of KNN as a non-parametric method — despite its simplicity, it handled the wiggly decision boundary well. This exercise also highlighted the importance of visualization and evaluation in understanding model behavior. Implementing KNN manually helped clarify how distances and neighborhood voting contribute to decisions, and comparing against a well-established library provided a useful sanity check. Overall, this was a valuable exploration of a foundational classification technique applied to a nontrivial problem.</p>
<div id="2f0df107" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Full implementation for 2a: K-Nearest Neighbors assignment</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 1. Generate training data --------</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>x1_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>x2_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>boundary_train <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1_train) <span class="op">+</span> x1_train</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> (x2_train <span class="op">&gt;</span> boundary_train).astype(<span class="bu">int</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.vstack((x1_train, x2_train)).T</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data with boundary</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(x1_train, x2_train, c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'bwr'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">300</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>boundary_vals <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x_vals) <span class="op">+</span> x_vals</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, boundary_vals, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Decision Boundary'</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x1"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"x2"</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training Data with Wiggly Boundary"</span>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 2. Generate test data with a different seed --------</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">99</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>x1_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>x2_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>boundary_test <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1_test) <span class="op">+</span> x1_test</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> (x2_test <span class="op">&gt;</span> boundary_test).astype(<span class="bu">int</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.vstack((x1_test, x2_test)).T</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 3. Custom KNN implementation --------</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_predict(X_train, y_train, X_test, k):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X_test:</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.linalg.norm(X_train <span class="op">-</span> x, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        nearest_indices <span class="op">=</span> np.argsort(distances)[:k]</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        nearest_labels <span class="op">=</span> y_train[nearest_indices]</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        majority_vote <span class="op">=</span> np.argmax(np.bincount(nearest_labels))</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        predictions.append(majority_vote)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 4. Evaluate over k = 1 to 30 --------</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>ks <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">31</span>)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>accuracy_custom <span class="op">=</span> []</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>accuracy_sklearn <span class="op">=</span> []</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> ks:</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    y_pred_custom <span class="op">=</span> knn_predict(X_train, y_train, X_test, k)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    acc_custom <span class="op">=</span> accuracy_score(y_test, y_pred_custom)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    accuracy_custom.append(acc_custom)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    y_pred_builtin <span class="op">=</span> model.predict(X_test)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    acc_builtin <span class="op">=</span> accuracy_score(y_test, y_pred_builtin)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    accuracy_sklearn.append(acc_builtin)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 5. Plot accuracy over k --------</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, accuracy_custom, label<span class="op">=</span><span class="st">'Custom KNN'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, accuracy_sklearn, label<span class="op">=</span><span class="st">'Sklearn KNN'</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Neighbors (k)'</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy on Test Set'</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN Accuracy vs. k'</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<!-- _todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._

_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._

_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._

_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_  -->
<!-- ## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._


 -->


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Machine Learning"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jun Zhu Li"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- _todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._ --&gt;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1a. K-Means</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- _todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._ --&gt;</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{python}</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">import numpy as np</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">import pandas as pd</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">import matplotlib.pyplot as plt</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">penguins = pd.read_csv("palmer_penguins.csv")  # Adjust path if needed</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co">X = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().values</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- K-Means Function -----------</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co">def initialize_centroids(X, k):</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co">    indices = np.random.choice(len(X), size=k, replace=False)</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co">    return X[indices]</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co">def assign_clusters(X, centroids):</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co">    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co">    return np.argmin(distances, axis=1)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co">def update_centroids(X, labels, k):</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co">    return np.array([X[labels == i].mean(axis=0) for i in range(k)])</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co">def k_means_with_tracking(X, k, max_iters=100, tol=1e-4):</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">    centroids = initialize_centroids(X, k)</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">    history = []</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="co">    for iteration in range(max_iters):</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">        labels = assign_clusters(X, centroids)</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">        new_centroids = update_centroids(X, labels, k)</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co">        history.append((centroids.copy(), labels.copy()))</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="co">        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) &lt; tol):</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="co">            break</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="co">        centroids = new_centroids</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="co">    return labels, centroids, history</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- Run Custom K-Means -----------</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co">k = 3</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="co">labels_custom, centroids_custom, history = k_means_with_tracking(X, k)</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------------- Plots ------------------</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="co">for step, (centroids_step, labels_step) in enumerate(history):</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.figure(figsize=(6, 5))</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="co">    for i in range(k):</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a><span class="co">        plt.scatter(X[labels_step == i][:, 0], X[labels_step == i][:, 1], label=f'Cluster {i+1}')</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.scatter(centroids_step[:, 0], centroids_step[:, 1], c='black', s=150, marker='X', label='Centroids')</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.title(f'K-Means Iteration {step + 1}')</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.xlabel('Bill Length (mm)')</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.ylabel('Flipper Length (mm)')</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.legend()</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.grid(True)</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.tight_layout()</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.show()</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="co">``` --&gt;</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>To explore the fundamentals of unsupervised learning, I implemented the K-Means clustering algorithm from scratch and applied it to the Palmer Penguins dataset. I specifically focused on two continuous and biologically meaningful features: bill length (in millimeters) and flipper length (in millimeters). These variables provide a useful basis for clustering because they exhibit noticeable differences across species and lend themselves well to two-dimensional visualization. My implementation of K-Means followed the classic structure: initializing random centroids, assigning each data point to the nearest centroid using Euclidean distance, updating centroids as the mean of their assigned points, and repeating this process until convergence. To enhance interpretability, I visualized the algorithm’s progress at each iteration, showing how clusters gradually formed and stabilized as the centroids moved and the point assignments changed. These visualizations provided a tangible understanding of how K-Means optimizes the placement of centroids and the grouping of data.</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>After confirming the correctness of my custom algorithm through visual inspection, I compared my results to Python’s built-in KMeans function from the sklearn.cluster module. The outputs from both implementations were nearly identical, reinforcing that my manual version performed as expected. With clusters that corresponded visually to three distinct groupings—likely reflecting the known Adelie, Gentoo, and Chinstrap species—it was clear that K-Means could discover structure in the data without supervision. Following this, I moved on to evaluate the optimal number of clusters using two widely accepted metrics: the within-cluster sum of squares (WCSS), which assesses compactness of clusters, and the silhouette score, which measures how well-separated the clusters are. I used my custom K-Means algorithm to generate cluster labels for each K value from 2 through 7, and then computed WCSS manually while using the built-in silhouette_score function from sklearn.metrics.</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>The results of this evaluation revealed several insights. WCSS decreased consistently as K increased, as expected, but the rate of decrease sharply diminished after K=3—producing the classic “elbow” shape that suggests an optimal K. Silhouette scores peaked at K=2, indicating the tightest and most well-separated clusters at that level, but remained relatively high at K=3. Beyond K=3, the silhouette scores declined more rapidly, particularly at K=6, suggesting that larger values of K may fragment the data too much and reduce cluster quality. These metrics jointly suggest that K=2 is statistically optimal, but K=3 offers a balance between model performance and interpretability. Since the dataset is known to contain three penguin species, K=3 is the most reasonable and domain-justified choice.</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>Overall, this analysis confirmed the power and intuition behind K-Means clustering. Writing the algorithm from scratch provided a deeper understanding of the mechanics behind clustering, while visualizing each iteration helped demystify the convergence process. The comparison with the built-in implementation validated my work, and the metric-based evaluation helped inform the most appropriate number of clusters. The combination of quantitative analysis and domain knowledge led to the conclusion that K=3 is the most interpretable and statistically justifiable clustering solution for the Palmer Penguins dataset.</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> imageio</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> pd.read_csv(<span class="st">"palmer_penguins.csv"</span>)  <span class="co"># Adjust path if needed</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">'bill_length_mm'</span>, <span class="st">'flipper_length_mm'</span>]].dropna().values</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- K-Means Function -----------</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_centroids(X, k):</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), size<span class="op">=</span>k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X[indices]</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> assign_clusters(X, centroids):</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>    distances <span class="op">=</span> np.linalg.norm(X[:, np.newaxis] <span class="op">-</span> centroids, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_centroids(X, labels, k):</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([X[labels <span class="op">==</span> i].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_means_with_tracking(X, k, max_iters<span class="op">=</span><span class="dv">100</span>, tol<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> initialize_centroids(X, k)</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> []</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> assign_clusters(X, centroids)</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>        new_centroids <span class="op">=</span> update_centroids(X, labels, k)</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>        history.append((centroids.copy(), labels.copy()))</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(np.linalg.norm(new_centroids <span class="op">-</span> centroids, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">&lt;</span> tol):</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> new_centroids</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels, centroids, history</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- Run Custom K-Means -----------</span></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>labels_custom, centroids_custom, history <span class="op">=</span> k_means_with_tracking(X, k)</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a><span class="co"># ----------- Create Animated GIF -----------</span></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">"kmeans_frames"</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>filenames <span class="op">=</span> []</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (centroids_step, labels_step) <span class="kw">in</span> <span class="bu">enumerate</span>(history):</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[labels_step <span class="op">==</span> i][:, <span class="dv">0</span>], X[labels_step <span class="op">==</span> i][:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>    ax.scatter(centroids_step[:, <span class="dv">0</span>], centroids_step[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">150</span>, marker<span class="op">=</span><span class="st">'X'</span>, label<span class="op">=</span><span class="st">'Centroids'</span>)</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'K-Means Iteration </span><span class="sc">{</span>step <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Bill Length (mm)'</span>)</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Flipper Length (mm)'</span>)</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a>    filename <span class="op">=</span> <span class="ss">f'kmeans_frames/frame_</span><span class="sc">{</span>step<span class="sc">:02d}</span><span class="ss">.png'</span></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a>    plt.savefig(filename)</span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a>    filenames.append(filename)</span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>gif_path <span class="op">=</span> <span class="st">'kmeans_clustering.gif'</span></span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> imageio.get_writer(gif_path, mode<span class="op">=</span><span class="st">'I'</span>, duration<span class="op">=</span><span class="fl">0.8</span>, loop<span class="op">=</span><span class="dv">0</span>) <span class="im">as</span> writer:</span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> filename <span class="kw">in</span> filenames:</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> imageio.imread(filename)</span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>        writer.append_data(image)</span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> filename <span class="kw">in</span> filenames:</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a>    os.remove(filename)</span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a><span class="co"># Final dummy assignment to suppress output (if needed)</span></span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a><span class="fu">## Clustering Animation</span></span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>Here’s how the K-Means algorithm iteratively converges:</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a><span class="al">![K-Means Clustering Animation](kmeans_clustering.gif)</span></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>K_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">8</span>)</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>wcss_values <span class="op">=</span> []</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>silhouette_scores <span class="op">=</span> []</span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> K_range:</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>    labels, centroids, _ <span class="op">=</span> k_means_with_tracking(X, k)</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute WCSS manually</span></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a>    wcss <span class="op">=</span> <span class="bu">sum</span>(np.<span class="bu">sum</span>((X[labels <span class="op">==</span> i] <span class="op">-</span> centroids[i])<span class="op">**</span><span class="dv">2</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k))</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a>    wcss_values.append(wcss)</span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute silhouette score using sklearn</span></span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>        sil_score <span class="op">=</span> silhouette_score(X, labels)</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append(sil_score)</span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append(np.nan)  <span class="co"># Not defined for k=1</span></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot WCSS and Silhouette Score</span></span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a><span class="co"># WCSS plot</span></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(K_range, wcss_values, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Within-Cluster Sum of Squares (WCSS)'</span>)</span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Number of Clusters (K)'</span>)</span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'WCSS'</span>)</span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>)</span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Silhouette Score plot</span></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(K_range, silhouette_scores, marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Silhouette Score'</span>)</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Clusters (K)'</span>)</span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Silhouette Score'</span>)</span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>)</span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ## 1b. Latent-Class MNL</span></span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura &amp; Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._</span></span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a><span class="co">_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._</span></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._</span></span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._</span></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._ --&gt;</span></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2a. K Nearest Neighbors</span></span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- _todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._ --&gt;</span></span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r}</span></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a><span class="co"># gen data -----</span></span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a><span class="co">set.seed(42)</span></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a><span class="co">n &lt;- 100</span></span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a><span class="co">x1 &lt;- runif(n, -3, 3)</span></span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a><span class="co">x2 &lt;- runif(n, -3, 3)</span></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a><span class="co">x &lt;- cbind(x1, x2)</span></span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a><span class="co"># define a wiggly boundary</span></span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a><span class="co">boundary &lt;- sin(4*x1) + x1</span></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a><span class="co">y &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()</span></span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a><span class="co">dat &lt;- data.frame(x1 = x1, x2 = x2, y = y)</span></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a><span class="co">``` --&gt;</span></span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- $\beta_\text{price}$ --&gt;</span></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a>I implemented and analyzed the K-Nearest Neighbors (KNN) algorithm using a fully synthetic dataset designed to challenge classifiers with a nonlinear decision boundary. The dataset was generated using two features, x1 and x2, each sampled uniformly between -3 and 3. Labels were assigned based on whether a point lay above or below a wiggly curve defined by the function $\text{x2 = sin(4x1) + x1}$, creating a binary classification task. The training data was visualized with the decision boundary clearly plotted, which provided an intuitive understanding of how difficult the classification would be for more rigid models. This plot showed red and blue classes (1 and 0) separated by a sinusoidal-like boundary, hinting at the complexity of the decision space.</span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a>To validate the generalization performance of the algorithm, I created a separate test set using the same process but a different random seed to ensure independence. With the data prepared, I implemented the KNN algorithm from scratch. The custom version computed Euclidean distances between each test point and all training points, identified the k closest neighbors, and made a prediction based on majority voting among their labels. This approach provided transparency into how each prediction was made, and reinforced my understanding of distance-based learning.</span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a>To evaluate the performance of my implementation, I compared it directly with the built-in KNeighborsClassifier from the scikit-learn library. For both versions of KNN, I iterated over values of k from 1 to 30, computing the percentage of correctly classified test points at each step. The resulting plot showed that both the custom and built-in versions tracked nearly identical accuracy curves, validating the correctness of my implementation. The highest accuracy occurred at very low k values, specifically around k = 1 or k = 2, and declined gradually as k increased. This is consistent with the nature of the problem — a highly nonlinear boundary requires a low-bias model like KNN with small k to closely fit the data’s shape.</span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a>The key insight from this exercise was the balance between flexibility and generalization in KNN. While a very small k can lead to overfitting in noisier datasets, in this case, the complexity of the true boundary made low k optimal. Another takeaway was the robustness and simplicity of KNN as a non-parametric method — despite its simplicity, it handled the wiggly decision boundary well. This exercise also highlighted the importance of visualization and evaluation in understanding model behavior. Implementing KNN manually helped clarify how distances and neighborhood voting contribute to decisions, and comparing against a well-established library provided a useful sanity check. Overall, this was a valuable exploration of a foundational classification technique applied to a nontrivial problem.</span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a><span class="co"># Full implementation for 2a: K-Nearest Neighbors assignment</span></span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 1. Generate training data --------</span></span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a>x1_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a>x2_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a>boundary_train <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1_train) <span class="op">+</span> x1_train</span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> (x2_train <span class="op">&gt;</span> boundary_train).astype(<span class="bu">int</span>)</span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.vstack((x1_train, x2_train)).T</span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data with boundary</span></span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">6</span>))</span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a>plt.scatter(x1_train, x2_train, c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'bwr'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">300</span>)</span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a>boundary_vals <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x_vals) <span class="op">+</span> x_vals</span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, boundary_vals, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Decision Boundary'</span>)</span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x1"</span>)</span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"x2"</span>)</span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training Data with Wiggly Boundary"</span>)</span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 2. Generate test data with a different seed --------</span></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">99</span>)</span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a>x1_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a>x2_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a>boundary_test <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1_test) <span class="op">+</span> x1_test</span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> (x2_test <span class="op">&gt;</span> boundary_test).astype(<span class="bu">int</span>)</span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.vstack((x1_test, x2_test)).T</span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 3. Custom KNN implementation --------</span></span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_predict(X_train, y_train, X_test, k):</span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X_test:</span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.linalg.norm(X_train <span class="op">-</span> x, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a>        nearest_indices <span class="op">=</span> np.argsort(distances)[:k]</span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a>        nearest_labels <span class="op">=</span> y_train[nearest_indices]</span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a>        majority_vote <span class="op">=</span> np.argmax(np.bincount(nearest_labels))</span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a>        predictions.append(majority_vote)</span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 4. Evaluate over k = 1 to 30 --------</span></span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a>ks <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">31</span>)</span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a>accuracy_custom <span class="op">=</span> []</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a>accuracy_sklearn <span class="op">=</span> []</span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> ks:</span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a>    y_pred_custom <span class="op">=</span> knn_predict(X_train, y_train, X_test, k)</span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a>    acc_custom <span class="op">=</span> accuracy_score(y_test, y_pred_custom)</span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a>    accuracy_custom.append(acc_custom)</span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a>    y_pred_builtin <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a>    acc_builtin <span class="op">=</span> accuracy_score(y_test, y_pred_builtin)</span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a>    accuracy_sklearn.append(acc_builtin)</span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a><span class="co"># -------- 5. Plot accuracy over k --------</span></span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, accuracy_custom, label<span class="op">=</span><span class="st">'Custom KNN'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, accuracy_sklearn, label<span class="op">=</span><span class="st">'Sklearn KNN'</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Neighbors (k)'</span>)</span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy on Test Set'</span>)</span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN Accuracy vs. k'</span>)</span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- _todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._</span></span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._</span></span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._</span></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_  --&gt;</span></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ## 2b. Key Drivers Analysis</span></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a><span class="co">_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_</span></span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a><span class="co">_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._</span></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a><span class="co"> --&gt;</span></span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>