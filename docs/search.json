[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jun Zhu Li",
    "section": "",
    "text": "I‚Äôm a passionate and dedicated professional experienced in Data Science, Web Development, Open Source Contributions, and Tech Writing. I thrive when working on creative, challenging projects and enjoy collaborating with others to solve exciting problems.\nExplore my Resume or dive into my Projects to learn more about my work and experiences!\n\nInterests\n\nüßë‚Äçüíª Data Science\nüåê Web Development\nü§ù Business Analyst\n‚úçÔ∏è Strategic Consultant\n\n\n\nüì´ Let‚Äôs Connect!\nI‚Äôm always open to discussing new ideas and opportunities. Reach out via email at jzhuuli@gmail.com or find me on LinkedIn."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nJun Zhu Li\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJun Zhu Li\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "Section 2: Analysis",
    "text": "Section 2: Analysis\nI analyzed the data\n\nimport matplotlib.pyplot as plt\n\n# Sample mtcars dataset\nwt = [2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.440,\n      3.440, 4.070, 3.730, 3.780, 5.250, 5.424, 5.345, 2.200, 1.615, 1.835,\n      2.465, 3.520, 3.435, 3.840, 3.845, 1.935, 2.140, 1.513, 3.170, 2.770,\n      3.570, 2.780]\n\nmpg = [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,\n       17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9,\n       21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26.0, 30.4, 15.8, 19.7,\n       15.0, 21.4]\n\n# Scatter plot\nplt.scatter(wt, mpg)\n\n# Labels and title\nplt.xlabel('Weight')\nplt.ylabel('Miles per Gallon (mpg)')\nplt.title('Scatter Plot of Weight vs. MPG')\n\n# Display plot\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "Section 2: Analysis",
    "text": "Section 2: Analysis\nI analyzed the data\n\nimport matplotlib.pyplot as plt\n\n# Sample mtcars dataset\nwt = [2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.440,\n      3.440, 4.070, 3.730, 3.780, 5.250, 5.424, 5.345, 2.200, 1.615, 1.835,\n      2.465, 3.520, 3.435, 3.840, 3.845, 1.935, 2.140, 1.513, 3.170, 2.770,\n      3.570, 2.780]\n\nmpg = [21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,\n       17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9,\n       21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26.0, 30.4, 15.8, 19.7,\n       15.0, 21.4]\n\n# Scatter plot\nplt.scatter(wt, mpg)\n\n# Labels and title\nplt.xlabel('Weight')\nplt.ylabel('Miles per Gallon (mpg)')\nplt.title('Scatter Plot of Weight vs. MPG')\n\n# Display plot\nplt.show()"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jun Zhu Li",
    "section": "About Me",
    "text": "About Me\nHi! I‚Äôm Jun Zhu Li, a passionate and dedicated professional with experience in [your field/expertise]. I love working on creative and challenging projects, continuously learning new skills, and collaborating with others to solve exciting problems.\nFeel free to explore my Resume or check out my Projects to learn more about my work!\n\nüöÄ Interests\n\nData Science\nWeb Development\nOpen Source Contributions\nTech Writing\n\n\n\nüì´ Get in Touch\nI‚Äôm always excited to discuss new opportunities and connect with interesting people. Reach out via email or connect with me on LinkedIn.\n\n\n# Quick Python Demo\nprint(\"Jun is the best!\")\n\nJun is the best!"
  },
  {
    "objectID": "index.html#hi-im-jun-zhu-li",
    "href": "index.html#hi-im-jun-zhu-li",
    "title": "Jun Zhu Li",
    "section": "",
    "text": "I‚Äôm a passionate and dedicated professional experienced in Data Science, Web Development, Open Source Contributions, and Tech Writing. I thrive when working on creative, challenging projects and enjoy collaborating with others to solve exciting problems.\nExplore my Resume or dive into my Projects to learn more about my work and experiences!\n\n\n\nüßë‚Äçüíª Data Science\nüåê Web Development\nü§ù Open Source Contributions\n‚úçÔ∏è Tech Writing\n\n\n\n\nI‚Äôm always open to discussing new ideas and opportunities. Reach out via email at jzhuuli@gmail.com or find me on LinkedIn."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html",
    "href": "blog/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nTo fully appreciate the importance and implications of this work, it is useful to examine the broader context in which the study was conducted, as well as the experimental methodology and key findings that emerged.\nIn the field of charitable fundraising, a persistent challenge faced by organizations is determining the most effective strategies to encourage donations. Historically, private giving in the United States has been robust, frequently surpassing 2% of GDP; however, fundraising practices have traditionally been guided more by anecdotal experiences and conventional wisdom rather than by rigorous empirical evidence. To address this gap, economists Dean Karlan (Yale University) and John List (University of Chicago) executed a pioneering field experiment aimed at empirically examining whether and how the ‚Äúprice‚Äù of giving influences charitable contributions.\nKarlan and List‚Äôs research specifically focused on the ‚Äúdemand side‚Äù of charitable donations, an area notably under-explored relative to the tax-driven ‚Äúsupply side‚Äù incentives. The core research question guiding their investigation was whether reducing the effective cost of charitable donations through matching grants could successfully increase donor contributions. To empirically test this question, the researchers collaborated with a liberal nonprofit organization in the United States, utilizing a sample of 50,083 prior donors. These individuals were randomly assigned into two primary groups: approximately one-third constituted the control group, receiving a standard four-page fundraising letter, while the remaining two-thirds formed the treatment group, receiving a similar letter that prominently included an offer of a matching grant.\nWithin the treatment group, further randomization led to the creation of specific sub-conditions. These conditions varied by matching ratios (1:1, 2:1, or 3:1), maximum match amounts ($25,000, $50,000, $100,000, or an unspecified cap), and suggested donation amounts relative to donors‚Äô previous contributions (100%, 125%, or 150%). The fundraising letters, distributed in August 2005, strategically referenced contemporary political events, specifically Supreme Court nominations, to ensure the real-world relevance and external validity of the study.\nThe results of Karlan and List‚Äôs experiment revealed compelling evidence regarding the efficacy of matching grants. The simple presence of a matching offer increased both the response rate by 22% and revenue per solicitation by 19%. Interestingly, however, increasing the match ratio beyond 1:1 to ratios of 2:1 or 3:1 yielded no additional incremental benefit. The findings suggest that the matching grants may have primarily acted as signals of credibility and urgency, leveraging social cues embedded in the framing of the match.\nAdditionally, the experiment found that variations in maximum match amounts and suggested donation benchmarks did not significantly influence donor behavior. Importantly, the study identified a notable contextual effect related to political environment: donors in politically conservative (‚Äúred‚Äù) states demonstrated significantly higher responsiveness to matching offers, with an increase of 55% in revenue per solicitation, whereas minimal effects were observed in politically liberal (‚Äúblue‚Äù) states. The estimated price elasticity derived from this experiment was approximately ‚Äì0.225, aligning well with prior elasticity estimates from studies examining tax-based incentives.\nThis research provides clear implications for fundraising strategy and policy. Practitioners are advised that matching grants are effective tools, though escalating match ratios beyond a simple 1:1 offer may not be cost-effective or necessary. Moreover, the study challenges the conventional assumption that larger matching ratios inherently generate greater donor enthusiasm. Finally, the observed sensitivity to political and social context suggests that fundraisers must carefully consider these factors when crafting appeals.\nKarlan and List‚Äôs seminal findings were published in their influential paper, ‚ÄúDoes Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment,‚Äù appearing in the American Economic Review in 2007. Their supporting data are publicly accessible through the AEA Data Archive, Innovations for Poverty Action (IPA), and Harvard Dataverse, facilitating further academic scrutiny and replication studies."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#introduction",
    "href": "blog/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nTo fully appreciate the importance and implications of this work, it is useful to examine the broader context in which the study was conducted, as well as the experimental methodology and key findings that emerged.\nIn the field of charitable fundraising, a persistent challenge faced by organizations is determining the most effective strategies to encourage donations. Historically, private giving in the United States has been robust, frequently surpassing 2% of GDP; however, fundraising practices have traditionally been guided more by anecdotal experiences and conventional wisdom rather than by rigorous empirical evidence. To address this gap, economists Dean Karlan (Yale University) and John List (University of Chicago) executed a pioneering field experiment aimed at empirically examining whether and how the ‚Äúprice‚Äù of giving influences charitable contributions.\nKarlan and List‚Äôs research specifically focused on the ‚Äúdemand side‚Äù of charitable donations, an area notably under-explored relative to the tax-driven ‚Äúsupply side‚Äù incentives. The core research question guiding their investigation was whether reducing the effective cost of charitable donations through matching grants could successfully increase donor contributions. To empirically test this question, the researchers collaborated with a liberal nonprofit organization in the United States, utilizing a sample of 50,083 prior donors. These individuals were randomly assigned into two primary groups: approximately one-third constituted the control group, receiving a standard four-page fundraising letter, while the remaining two-thirds formed the treatment group, receiving a similar letter that prominently included an offer of a matching grant.\nWithin the treatment group, further randomization led to the creation of specific sub-conditions. These conditions varied by matching ratios (1:1, 2:1, or 3:1), maximum match amounts ($25,000, $50,000, $100,000, or an unspecified cap), and suggested donation amounts relative to donors‚Äô previous contributions (100%, 125%, or 150%). The fundraising letters, distributed in August 2005, strategically referenced contemporary political events, specifically Supreme Court nominations, to ensure the real-world relevance and external validity of the study.\nThe results of Karlan and List‚Äôs experiment revealed compelling evidence regarding the efficacy of matching grants. The simple presence of a matching offer increased both the response rate by 22% and revenue per solicitation by 19%. Interestingly, however, increasing the match ratio beyond 1:1 to ratios of 2:1 or 3:1 yielded no additional incremental benefit. The findings suggest that the matching grants may have primarily acted as signals of credibility and urgency, leveraging social cues embedded in the framing of the match.\nAdditionally, the experiment found that variations in maximum match amounts and suggested donation benchmarks did not significantly influence donor behavior. Importantly, the study identified a notable contextual effect related to political environment: donors in politically conservative (‚Äúred‚Äù) states demonstrated significantly higher responsiveness to matching offers, with an increase of 55% in revenue per solicitation, whereas minimal effects were observed in politically liberal (‚Äúblue‚Äù) states. The estimated price elasticity derived from this experiment was approximately ‚Äì0.225, aligning well with prior elasticity estimates from studies examining tax-based incentives.\nThis research provides clear implications for fundraising strategy and policy. Practitioners are advised that matching grants are effective tools, though escalating match ratios beyond a simple 1:1 offer may not be cost-effective or necessary. Moreover, the study challenges the conventional assumption that larger matching ratios inherently generate greater donor enthusiasm. Finally, the observed sensitivity to political and social context suggests that fundraisers must carefully consider these factors when crafting appeals.\nKarlan and List‚Äôs seminal findings were published in their influential paper, ‚ÄúDoes Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment,‚Äù appearing in the American Economic Review in 2007. Their supporting data are publicly accessible through the AEA Data Archive, Innovations for Poverty Action (IPA), and Harvard Dataverse, facilitating further academic scrutiny and replication studies."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#data",
    "href": "blog/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset compiled by Dean Karlan and John List for their 2007 field experiment comprises 50,083 observations, each corresponding to an individual who had previously donated to a U.S.-based nonprofit organization. The data were collected to evaluate the effectiveness of different fundraising appeals and include detailed information on treatment assignments, donor responses, demographic characteristics, and contextual political variables. Approximately two-thirds of the sample were assigned to a treatment group that received a fundraising letter featuring a matching grant offer, with match ratios randomly varied across 1:1, 2:1, and 3:1 levels. The remaining third received a standard appeal letter and served as the control group.\nAdditional randomization was applied within the treatment group to vary the maximum matching amount ($25,000, $50,000, $100,000, or unstated) and the suggested donation amount based on the donor‚Äôs past giving behavior. The dataset includes binary indicators of whether a donation was made in response to the solicitation, the amount contributed, and the change in donation relative to previous behavior.\nFurthermore, it contains socio-demographic and contextual variables at the ZIP code level, including average household size, median household income, homeownership rate, educational attainment, and the proportion of residents in urban areas. Political context is captured through indicators of whether the respondent resided in a red or blue state or county, based on the 2004 presidential election. This rich, multi-dimensional dataset enables rigorous analysis of causal effects in charitable giving, while also allowing for the exploration of heterogeneous treatment effects across geographic, political, and demographic lines.\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.describe\n\n&lt;bound method NDFrame.describe of        treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0              0        1  Control       0       0   Control       0       0   \n1              0        1  Control       0       0   Control       0       0   \n2              1        0        1       0       0  $100,000       0       0   \n3              1        0        1       0       0  Unstated       0       0   \n4              1        0        1       0       0   $50,000       0       1   \n...          ...      ...      ...     ...     ...       ...     ...     ...   \n50078          1        0        1       0       0   $25,000       1       0   \n50079          0        1  Control       0       0   Control       0       0   \n50080          0        1  Control       0       0   Control       0       0   \n50081          1        0        3       0       1  Unstated       0       0   \n50082          1        0        3       0       1   $25,000       1       0   \n\n       size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0            0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1            0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2            1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3            0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4            0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n...        ...     ...  ...    ...      ...       ...       ...        ...   \n50078        0       0  ...    0.0      1.0  0.872797  0.089959   0.257265   \n50079        0       0  ...    0.0      1.0  0.688262  0.108889   0.288792   \n50080        0       0  ...    1.0      0.0  0.900000  0.021311   0.178689   \n50081        0       1  ...    1.0      0.0  0.917206  0.008257   0.225619   \n50082        0       0  ...    0.0      1.0  0.530023  0.074112   0.340698   \n\n       ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0           2.10          28517.0  0.499807      0.324528       1.000000  \n1            NaN              NaN       NaN           NaN            NaN  \n2           2.48          51175.0  0.721941      0.192668       1.000000  \n3           2.65          79269.0  0.920431      0.412142       1.000000  \n4           1.85          40908.0  0.416072      0.439965       1.000000  \n...          ...              ...       ...           ...            ...  \n50078       2.13          45047.0  0.771316      0.263744       1.000000  \n50079       2.67          74655.0  0.741931      0.586466       1.000000  \n50080       2.36          26667.0  0.778689      0.107930       0.000000  \n50081       2.57          39530.0  0.733988      0.184768       0.634903  \n50082       3.70          48744.0  0.717843      0.127941       0.994181  \n\n[50083 rows x 51 columns]&gt;\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nvariables_to_test = ['hpa', 'freq', 'years', 'female', 'red0']\nresults = {}\n\nfor var in variables_to_test:\n    treatment_data = df[df['treatment'] == 1][var].dropna()\n    control_data = df[df['control'] == 1][var].dropna()\n    \n    # T-test calculations\n    mean_diff = treatment_data.mean() - control_data.mean()\n    n_treat = len(treatment_data)\n    n_control = len(control_data)\n    var_treat = treatment_data.var(ddof=1)\n    var_control = control_data.var(ddof=1)\n    se_diff = ((var_treat / n_treat) + (var_control / n_control)) ** 0.5\n    t_stat = mean_diff / se_diff\n    df_denom = ((var_treat / n_treat)**2 / (n_treat - 1)) + ((var_control / n_control)**2 / (n_control - 1))\n    df_effective = (var_treat / n_treat + var_control / n_control)**2 / df_denom\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_effective))\n    \n    # Linear regression\n    y = df[var]\n    X = sm.add_constant(df['treatment'])\n    model = sm.OLS(y, X, missing='drop').fit()\n    reg_coef = model.params['treatment']\n    reg_pval = model.pvalues['treatment']\n    \n    # Store results\n    results[var] = {\n        'Mean (Treatment)': round(treatment_data.mean(), 4),\n        'Mean (Control)': round(control_data.mean(), 4),\n        'Difference': round(mean_diff, 4),\n        't-statistic': round(t_stat, 4),\n        'df': round(df_effective, 1),\n        'p-value (t-test)': round(p_value, 4),\n        'Regression Coef': round(reg_coef, 4),\n        'p-value (reg)': round(reg_pval, 4)\n    }\n\nresults_df = pd.DataFrame(results).T\nresults_df.index.name = 'Variable'\nresults_df\n\n\n\n\n\n\n\n\nMean (Treatment)\nMean (Control)\nDifference\nt-statistic\ndf\np-value (t-test)\nRegression Coef\np-value (reg)\n\n\nVariable\n\n\n\n\n\n\n\n\n\n\n\n\nhpa\n59.597198\n58.960201\n0.6371\n0.9704\n35913.898438\n0.3318\n0.6371\n0.3451\n\n\nfreq\n8.035400\n8.047300\n-0.0120\n-0.1108\n33326.400000\n0.9117\n-0.0120\n0.9117\n\n\nyears\n6.078400\n6.135900\n-0.0575\n-1.0909\n32400.900000\n0.2753\n-0.0575\n0.2700\n\n\nfemale\n0.275200\n0.282700\n-0.0075\n-1.7535\n32450.800000\n0.0795\n-0.0075\n0.0787\n\n\nred0\n0.407400\n0.398600\n0.0087\n1.8773\n33450.500000\n0.0605\n0.0087\n0.0608\n\n\n\n\n\n\n\nTo assess the integrity of the randomization process employed in Karlan and List‚Äôs (2007) field experiment, we conducted a series of balance tests comparing the treatment and control groups on several key pre-treatment variables. The goal of these tests is to verify that the random assignment of fundraising letters resulted in comparable groups with respect to observable characteristics prior to treatment, thereby strengthening the causal interpretation of the experimental findings.\nThe variables selected for the balance tests closely mirror those presented in Table 1 of the original publication and encompass both donation history and demographic attributes. Specifically, we examine (1) the donor‚Äôs highest previous contribution (hpa), (2) the number of prior donations (freq), (3) the number of years since the donor‚Äôs initial contribution to the organization (years), (4) an indicator variable for female donors (female), and (5) a binary indicator for residing in a politically conservative (‚Äúred‚Äù) state as determined by support for George W. Bush in the 2004 U.S. presidential election (red0).\nWhen comparing our calculated group means to those reported in Karlan and List‚Äôs Table 1, we find a near-exact match across all variables. In our sample, the mean highest previous contribution is $59.60 for the treatment group and $58.96 for the control group‚Äîidentical to the values in the published table. Likewise, the average number of prior donations is 8.04 in the treatment group and 8.05 in the control group, with years since initial donation averaging 6.08 and 6.14, respectively. The proportion of female donors is 27.5% in the treatment group and 28.3% in the control group, while the share of donors living in red states is 40.7% versus 39.9% across the two groups. These findings confirm that the implementation of the randomization closely aligns with the authors‚Äô original execution.\nTo formally test for balance, we employed both two-sample t-tests and simple linear regressions of each covariate on the treatment assignment indicator. In all cases, the resulting p-values were above the conventional 0.05 threshold, indicating no statistically significant differences between the treatment and control groups at the 95% confidence level. The closest results to marginal significance were observed for the female variable (p = 0.080 in the t-test; p = 0.079 in the regression) and the red0 variable (p = 0.060 in the t-test; p = 0.061 in the regression). While these values approach the 10% significance level, they are insufficient to suggest meaningful imbalance and instead support the interpretation that randomization achieved statistical equivalence on observed characteristics.\nThese balance checks serve a vital role in reinforcing the credibility of the experimental design. As underscored by Karlan and List in their original presentation of Table 1, demonstrating that treatment and control groups are comparable across a range of relevant baseline variables helps rule out confounding factors. Consequently, we can attribute any subsequent differences in donor behavior to the experimental treatments rather than to pre-existing group disparities."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#experimental-results",
    "href": "blog/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ndf['donated'] = df['gave'].fillna(0)\n\ndonation_rates = df.groupby('treatment')['donated'].mean()\n\nplt.figure(figsize=(6, 4))\n\ndf['donated'] = df['gave'].fillna(0)\n\ndonation_rates = df.groupby('treatment')['donated'].mean()\n\nplt.figure(figsize=(6, 4))\nsns.barplot(x=donation_rates.index, y=donation_rates.values)\nplt.xticks([0, 1], ['Control', 'Treatment'])\nplt.ylabel('Proportion Who Donated')\nplt.title('Donation Rates by Treatment Group')\nplt.ylim(0, max(donation_rates.values) * 1.2)\nplt.tight_layout()\nplt.grid(axis='y')\nplt.show()\n\ntreatment_don = df[df['treatment'] == 1]['donated']\ncontrol_don = df[df['treatment'] == 0]['donated']\nt_stat, p_val = stats.ttest_ind(treatment_don, control_don, equal_var=False)\n\nX_lin = sm.add_constant(df['treatment'])\nmodel_lin = sm.OLS(df['donated'], X_lin).fit()\n\nmodel_probit = sm.Probit(df['donated'], X_lin).fit(disp=0)\n\n# Create a DataFrame to display results in a table\nresults_table = pd.DataFrame({\n    'Control Group': [donation_rates[0]],\n    'Treatment Group': [donation_rates[1]],\n    'Difference': [donation_rates[1] - donation_rates[0]],\n    't-statistic': [t_stat],\n    'p-value (t-test)': [p_val],\n    'Linear Regression Coef': [model_lin.params['treatment']],\n    'p-value (regression)': [model_lin.pvalues['treatment']],\n    'Probit Coef': [model_probit.params['treatment']],\n    'p-value (probit)': [model_probit.pvalues['treatment']]\n})\n\nresults_table = results_table.round(4)\nresults_table.index = ['Donation Rate']\nresults_table\n\n&lt;Figure size 576x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl Group\nTreatment Group\nDifference\nt-statistic\np-value (t-test)\nLinear Regression Coef\np-value (regression)\nProbit Coef\np-value (probit)\n\n\n\n\nDonation Rate\n0.0179\n0.022\n0.0042\n3.2095\n0.0013\n0.0042\n0.0019\n0.0868\n0.0019\n\n\n\n\n\n\n\nTo evaluate the impact of matching grants on charitable giving, we analyzed the difference in donation rates between individuals who received a standard solicitation letter (control group) and those who received a letter offering a matching grant (treatment group). The results, visualized in Figure 1, show that 1.79% of individuals in the control group made a donation, compared to 2.20% in the treatment group. This initial comparison suggests a positive effect of the matching grant offer on the likelihood of donating.\nTo assess the statistical significance of this difference, we conducted a two-sample t-test for proportions. The t-statistic was 3.21, with a corresponding p-value of 0.0013, indicating that the increase in donation rates among the treatment group is statistically significant at the 1% level. This result suggests that the observed difference is unlikely to be due to random chance alone.\nTo further substantiate this finding, we estimated a bivariate linear regression with a binary dependent variable indicating whether a donation was made. The treatment group indicator had a coefficient of 0.0042, which implies a 0.42 percentage point increase in donation likelihood associated with receiving a matching offer. The p-value associated with this coefficient was 0.0019, reaffirming the statistical significance of the treatment effect. The model‚Äôs R-squared value, although small (0.00019), is typical for binary outcomes in field settings and highlights that the variation in donations is largely driven by unobserved factors, as expected.\nAdditionally, we replicated the probit regression reported in Table 3, Column 1 of Karlan and List (2007), where the probability of donating was modeled as a function of treatment status. Our probit estimate of the treatment coefficient was 0.087, with a p-value of 0.0019‚Äîclosely matching the original study. This replication reinforces the original conclusion that offering a matching grant significantly increases the probability of donation.\nTaken together, these results provide robust evidence that matching grants effectively increase charitable giving, even when evaluated using conservative statistical methods. The increase, though modest in magnitude, is statistically significant and practically meaningful, particularly in the context of large-scale fundraising efforts. These findings align with the behavioral insight that individuals are more likely to give when their contributions are perceived to have enhanced impact or when social cues, such as matching gifts, signal urgency or endorsement.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\nresponse_rates = treatment_df.groupby('ratio', observed=True)['donated'].mean()\n\nt1_vs_t2 = stats.ttest_ind(\n    treatment_df[treatment_df['ratio'] == 1]['donated'],\n    treatment_df[treatment_df['ratio'] == 2]['donated'],\n    equal_var=False\n)\n\nt2_vs_t3 = stats.ttest_ind(\n    treatment_df[treatment_df['ratio'] == 2]['donated'],\n    treatment_df[treatment_df['ratio'] == 3]['donated'],\n    equal_var=False\n)\n\nX_ratios = treatment_df[['ratio1', 'ratio2', 'ratio3']]\nmodel_ratios = sm.OLS(treatment_df['donated'], X_ratios).fit()\n\ncoef_1v2 = model_ratios.params['ratio2'] - model_ratios.params['ratio1']\ncoef_2v3 = model_ratios.params['ratio3'] - model_ratios.params['ratio2']\n\nvcov = model_ratios.cov_params()\nse_1v2 = (vcov.loc['ratio1', 'ratio1'] + vcov.loc['ratio2', 'ratio2'] - 2 * vcov.loc['ratio1', 'ratio2']) ** 0.5\nse_2v3 = (vcov.loc['ratio2', 'ratio2'] + vcov.loc['ratio3', 'ratio3'] - 2 * vcov.loc['ratio2', 'ratio3']) ** 0.5\n\nresults_df = pd.DataFrame({\n    'Match Ratio': ['1:1', '2:1', '3:1'],\n    'Response Rate': [response_rates[1], response_rates[2], response_rates[3]],\n    'Regression Coefficient': [model_ratios.params['ratio1'], model_ratios.params['ratio2'], model_ratios.params['ratio3']],\n    'p-value': [model_ratios.pvalues['ratio1'], model_ratios.pvalues['ratio2'], model_ratios.pvalues['ratio3']]\n})\n\ncomparison_df = pd.DataFrame({\n    'Comparison': ['1:1 vs 2:1', '2:1 vs 3:1'],\n    'Difference in Response Rate': [response_rates[2] - response_rates[1], response_rates[3] - response_rates[2]],\n    't-statistic': [t1_vs_t2.statistic, t2_vs_t3.statistic],\n    'p-value': [t1_vs_t2.pvalue, t2_vs_t3.pvalue],\n    'Coefficient Difference': [coef_1v2, coef_2v3],\n    'Standard Error': [se_1v2, se_2v3]\n})\n\nresults_df = results_df.round(4)\ncomparison_df = comparison_df.round(4)\n\nresults_df.set_index('Match Ratio')\n\nprint(results_df)\nprint(comparison_df)\n\n  Match Ratio  Response Rate  Regression Coefficient  p-value\n0         1:1         0.0207                  0.0207      0.0\n1         2:1         0.0226                  0.0226      0.0\n2         3:1         0.0227                  0.0227      0.0\n   Comparison  Difference in Response Rate  t-statistic  p-value  \\\n0  1:1 vs 2:1                       0.0019      -0.9650   0.3345   \n1  2:1 vs 3:1                       0.0001      -0.0501   0.9600   \n\n   Coefficient Difference  Standard Error  \n0                  0.0019           0.002  \n1                  0.0001           0.002  \n\n\nTo further investigate the effectiveness of different matching grant ratios, we restricted our analysis to individuals in the treatment group and examined variation in donation rates across three matching levels: 1:1, 2:1, and 3:1. The response rates were remarkably similar across groups, with 2.07% of individuals in the 1:1 match group donating, compared to 2.26% in the 2:1 group and 2.27% in the 3:1 group. Although there is a slight upward trend in donation rates as the match ratio increases, the absolute differences are marginal‚Äîonly 0.19 and 0.01 percentage points between successive levels.\nTo assess whether these differences are statistically meaningful, we conducted a series of two-sample t-tests comparing the donation rates between match levels. The difference between the 1:1 and 2:1 groups was not statistically significant (p=0.335), nor was the difference between the 2:1 and 3:1 groups (p=0.960). These results provide no evidence that offering a higher match ratio leads to a greater likelihood of donation.\nWe corroborated these findings using an ordinary least squares (OLS) regression, regressing the binary donation outcome on dummy indicators for each match ratio group. The regression was specified without an intercept so that each coefficient would represent the mean response rate for its respective group. The estimated coefficients were 0.02075 for the 1:1 group, 0.02263 for the 2:1 group, and 0.02273 for the 3:1 group, all statistically significant at the 0.001 level. However, differences between these coefficients were small and statistically indistinguishable. Specifically, the difference between the 2:1 and 1:1 coefficients was 0.00188 (SE = 0.00197), and the difference between the 3:1 and 2:1 coefficients was just 0.00010 (SE = 0.00197). Neither difference is statistically significant, confirming the t-test results.\nThese findings align with Karlan and List‚Äôs original interpretation in their 2007 study. While the presence of a match clearly boosts giving relative to a control condition, increasing the match ratio from 1:1 to 2:1 or 3:1 does not generate additional giving. This suggests that the behavioral mechanism driving increased donations is not necessarily the financial leverage offered by higher match ratios, but rather the psychological or social signal conveyed by the presence of any match offer. The match may function more as a nudge or cue, reinforcing the importance or credibility of the appeal, rather than as a precise price incentive.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nX_full = sm.add_constant(df['treatment'])\nmodel_full = sm.OLS(df['amount'], X_full, missing='drop').fit()\n\ndonors_df = df[df['amount'] &gt; 0].copy()\nX_donors = sm.add_constant(donors_df['treatment'])\nmodel_donors = sm.OLS(donors_df['amount'], X_donors).fit()\n\ntreatment_donors = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_donors = donors_df[donors_df['treatment'] == 0]['amount']\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, edgecolor='black')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Donation Amounts: Control Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nplt.hist(treatment_donors, bins=30, alpha=0.7, edgecolor='black')\nplt.axvline(treatment_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Donation Amounts: Treatment Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\nregression_results = pd.DataFrame({\n    'Model': ['OLS Full Sample', 'OLS Donors Only'],\n    'Coefficient': [model_full.params['treatment'], model_donors.params['treatment']],\n    'p-value': [model_full.pvalues['treatment'], model_donors.pvalues['treatment']]\n})\nregression_results.set_index('Model').round(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\np-value\n\n\nModel\n\n\n\n\n\n\nOLS Full Sample\n0.154\n0.063\n\n\nOLS Donors Only\n-1.668\n0.561\n\n\n\n\n\n\n\nTo evaluate the effect of matching grant offers on the amount donated, we conducted a series of ordinary least squares (OLS) regressions and visualized the distribution of donation amounts across treatment conditions. We first estimated a linear model using the full sample, which included both donors and non-donors. The regression specification modeled donation amount as a function of a binary treatment indicator. The estimated coefficient on the treatment variable was $0.154, with a p-value of 0.063. Although this estimate is not statistically significant at the conventional 5% level, it reaches marginal significance at the 10% level, suggesting that offering a matching grant may slightly increase overall donations by increasing participation, rather than the amount given by each donor.\nTo isolate the intensive margin, whether the treatment affected how much individuals gave once they decided to donate we ran the same regression on a restricted sample of donors only. In this case, the estimated coefficient on the treatment indicator was ‚Äì$1.67, with a p-value of 0.561. This result indicates that among those who chose to donate, individuals in the treatment group gave slightly less on average than those in the control group. However, the difference is not statistically significant, and the relatively large p-value suggests that this finding is likely attributable to random variation rather than a treatment effect.\nHistograms of donation amounts for both the control and treatment groups further illuminate the distributional patterns. As is typical in charitable giving data, both distributions are highly right-skewed, with most donors giving relatively small amounts and a few contributing substantially more. Visual inspection shows little difference between the two groups, with the average donation in each group marked by a red dashed line. The near overlap in mean values visually reinforces the regression results: while the presence of a matching offer may influence whether someone donates, it appears to have little effect on how much they give once they have made the decision to contribute.\nOverall, this analysis complements prior findings by reinforcing the notion that matching grants primarily operate on the extensive margin‚Äîthat is, increasing the likelihood of giving‚Äîrather than on the intensive margin, or the size of the gift. This distinction is important for fundraisers: while match offers can broaden donor participation, they may not significantly raise per-donor revenue among those who are already inclined to give."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#simulation-experiment",
    "href": "blog/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nn_simulations = 10000\nsample_sizes = [50, 100, 500, 1000]\n\nsim_results = {}\n\nfor n in sample_sizes:\n    control_samples = np.random.binomial(1, p_control, size=(n_simulations, n))\n    treatment_samples = np.random.binomial(1, p_treatment, size=(n_simulations, n))\n\n    control_means = control_samples.mean(axis=1)\n    treatment_means = treatment_samples.mean(axis=1)\n\n    pooled_std = np.sqrt((control_means * (1 - control_means) / n) +\n                         (treatment_means * (1 - treatment_means) / n))\n    \n    valid_indices = pooled_std &gt; 0\n    t_stats = np.full(n_simulations, np.nan) \n    t_stats[valid_indices] = (treatment_means[valid_indices] - control_means[valid_indices]) / pooled_std[valid_indices]\n\n    sim_results[n] = {\n        \"mean_control\": np.mean(control_means),\n        \"mean_treatment\": np.mean(treatment_means),\n        \"mean_t_statistic\": np.nanmean(t_stats),\n        \"std_t_statistic\": np.nanstd(t_stats),\n        \"t_statistics\": t_stats\n    }\n\nsim_results_df = pd.DataFrame({\n    \"Sample Size\": sample_sizes,\n    \"Mean (Control)\": [sim_results[n][\"mean_control\"] for n in sample_sizes],\n    \"Mean (Treatment)\": [sim_results[n][\"mean_treatment\"] for n in sample_sizes],\n    \"Mean t-stat\": [sim_results[n][\"mean_t_statistic\"] for n in sample_sizes],\n    \"SD t-stat\": [sim_results[n][\"std_t_statistic\"] for n in sample_sizes]\n})\nsim_results_df.set_index(\"Sample Size\").round(5)\n\n\n\n\n\n\n\n\nMean (Control)\nMean (Treatment)\nMean t-stat\nSD t-stat\n\n\nSample Size\n\n\n\n\n\n\n\n\n50\n0.01797\n0.02230\n0.16417\n1.01288\n\n\n100\n0.01792\n0.02204\n0.20872\n0.99637\n\n\n500\n0.01799\n0.02204\n0.45369\n1.00300\n\n\n1000\n0.01800\n0.02195\n0.62989\n0.98994\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nfor i, n in enumerate([500, 1000]):\n    t_stats = sim_results[n]['t_statistics']\n    axs[i].hist(t_stats, bins=50, alpha=0.7, edgecolor='black', density=True)\n    axs[i].axvline(x=np.mean(t_stats), color='red', linestyle='--', label='Mean t-stat')\n    axs[i].set_title(f'T-statistics Distribution (n = {n})')\n    axs[i].set_xlabel('t-statistic')\n    axs[i].set_ylabel('Density')\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo illustrate foundational statistical principles such as the Law of Large Numbers and the Central Limit Theorem within the context of the Karlan and List (2007) experimental design, we conducted a simulation study comparing two groups: a control group with a donation probability of 0.018, and a treatment group with a slightly higher donation probability of 0.022. These probabilities mirror the actual donation rates observed in the original field experiment. We simulated 10,000 random samples for each group across a range of increasing sample sizes‚Äîspecifically n= 50, 100, 500, and 1,000 and calculated the mean donation rates as well as t-statistics for comparing group means within each simulation iteration.\nThe results underscore key statistical principles. As sample sizes increased, the simulated group means converged to the true underlying probabilities. For instance, at n=1,000, the average simulated donation rate for the control group was 0.01800, and for the treatment group, 0.02195‚Äînearly identical to their theoretical values. The average t-statistic for n=1,000 was approximately 0.63, with a standard deviation near 0.99, indicating the t-statistic distribution was both centered and scaled appropriately, as expected under the Central Limit Theorem. In contrast, at smaller sample sizes (n=50 and n=100), the t-statistic distributions were unstable, with many values undefined or highly variable due to the small number of binary outcomes and low donation rates. These issues resulted in missing or non-informative summary statistics, highlighting the volatility of inference at small ùëõ.\nVisualizations of the t-statistics for n=500 and n=1,000 further reinforce these patterns. The histograms reveal increasingly symmetric, bell-shaped distributions as the sample size grows, with the mean of each distribution marked by a red dashed line. At n=1,000, the distribution closely approximates a standard normal curve, a hallmark of the Central Limit Theorem. The standard deviation of the t-statistic distribution also approaches 1, which is consistent with theoretical expectations for standardized test statistics under the null hypothesis.\nThese results demonstrate how both the Law of Large Numbers and the Central Limit Theorem manifest in practice. As sample sizes grow, sample means become more accurate estimates of the population parameters (convergence in probability), and the sampling distribution of test statistics approaches a normal distribution, enabling valid inference using tools like the t-test. Conversely, in small samples, randomness dominates, increasing the likelihood of erratic or misleading results. This simulation thus reinforces the importance of sufficient sample size in experimental design and statistical inference‚Äîespecially when working with low-probability binary outcomes, such as charitable giving.\n\nLaw of Large Numbers\n\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(1, 0.018, size=100000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\ncontrol_draws_subset = control_draws[:10000]\n\ndiff_vector = treatment_draws - control_draws_subset\n\ncumulative_avg_diff = np.cumsum(diff_vector) / np.arange(1, len(diff_vector) + 1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Average of Differences')\nplt.axhline(y=0.022 - 0.018, color='red', linestyle='--', label='True Mean Difference (0.004)')\nplt.title('Law of Large Numbers: Cumulative Average of Differences')\nplt.xlabel('Number of Simulated Observations')\nplt.ylabel('Cumulative Average')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo visually demonstrate the Law of Large Numbers (LLN) in the context of donation behavior, we simulated binary donation outcomes from two Bernoulli distributions. Specifically, we generated 100,000 independent draws from a control group distribution with a probability of success p=0.018 and 10,000 draws from a treatment group distribution with p=0.022. We then computed the difference between each of the first 10,000 matched treatment and control observations and tracked the cumulative average of these differences across the sample. This cumulative average was plotted against the number of simulated observations, with a red dashed line indicating the true mean difference between the two groups (0.022 ‚àí 0.018 = 0.004).\nThe resulting graph illustrates the fundamental property of the LLN. In the early stages of the simulation, the cumulative average of differences fluctuates substantially, reflecting the volatility inherent in small samples. However, as more observations are included, the cumulative average begins to stabilize and gradually converges toward the true population difference of 0.004. This convergence visually affirms the core prediction of the LLN: as the number of independent and identically distributed trials increases, the sample average tends to approach the expected value.\nThis simulation effectively contextualizes the LLN using a real-world experimental design and underscores why statistical inference becomes more reliable with larger sample sizes. By showing how the empirical difference in donation behavior becomes increasingly stable and reflective of the underlying population parameters, the simulation reinforces both the theoretical importance and practical implications of the LLN in experimental economics and applied statistics.\n\n\nCentral Limit Theorem\n\nsample_sizes = [50, 200, 500, 1000]\nn_replications = 1000\np_control = 0.018\np_treatment = 0.022\n\nmean_diffs_by_n = {}\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(n_replications):\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n        mean_diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(mean_diff)\n    \n    mean_diffs_by_n[n] = mean_diffs\n    \n    ax = axs[i // 2, i % 2]\n    ax.hist(mean_diffs, bins=30, edgecolor='black', alpha=0.7, density=True)\n    ax.axvline(x=np.mean(mean_diffs), color='red', linestyle='--', label='Mean Difference')\n    ax.axvline(x=0, color='black', linestyle=':', label='Zero')\n    ax.set_title(f'Sample Size = {n}')\n    ax.set_xlabel('Average Difference (Treatment - Control)')\n    ax.set_ylabel('Density')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo illustrate the Central Limit Theorem (CLT) in the context of experimental data on charitable giving, we conducted a simulation comparing average donation rates between control and treatment groups drawn from Bernoulli distributions. Specifically, we simulated 1,000 replications at each of four different sample sizes n=50, 200, 500, and 1,000‚Äî sing control group probabilities of p=0.018 and treatment group probabilities of p=0.022. In each replication, we calculated the difference in sample means (treatment minus control), resulting in distributions of 1,000 average differences for each sample size.\nThe histograms of these average differences provide compelling visual evidence for the CLT. At n=50, the distribution is jagged and displays high variability. The value of zero, which represents the null hypothesis of no treatment effect, appears near the center of the distribution due to this noise. At n=200, the distribution becomes more symmetric and begins to resemble a bell-shaped curve, while the sampling mean visibly approaches the true population difference of 0.004. By n=500 and ùëõ=1,000, the distributions exhibit the hallmark shape of the normal distribution: smooth, symmetric, and tightly centered around the expected mean. In these larger samples, the value of zero lies clearly in the tail of the distribution, suggesting that a t-test would likely reject the null hypothesis in favor of a statistically significant treatment effect.\nThese results vividly demonstrate the predictive power of the CLT. As the sample size increases, the sampling distribution of the difference in sample means converges in shape to the normal distribution, even though the underlying data are binary. This convergence underpins the validity of inferential tools such as the t-test and confidence intervals in empirical work. It also highlights why larger sample sizes enhance statistical power and reduce the risk of Type II error. In sum, this simulation confirms that even in experiments involving binary outcomes like donation behavior, standard statistical inference remains appropriate and robust when sample sizes are sufficiently large."
  },
  {
    "objectID": "blog/hw1/main.html",
    "href": "blog/hw1/main.html",
    "title": "Introduction",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\n# ### Introduction\n\n# # Expanded Description of Karlan and List‚Äôs Fundraising Experiment\n# ## Context and Purpose\n\n# In the world of charitable fundraising, organizations often face a core challenge: how to effectively encourage people to donate. While private giving in the U.S. has historically been robust‚Äîoften exceeding 2% of GDP‚Äîfundraisers have long relied on anecdotes and rules of thumb rather than scientific evidence to guide their strategies. \n\n# To bring empirical clarity to this issue, economists **Dean Karlan** (Yale University) and **John List** (University of Chicago) conducted a landmark field experiment to investigate whether and how the *\"price\"* of giving affects charitable donations.\n\n# The study focused on understanding the *demand side* of charitable giving, which is less studied compared to the tax-incentivized *supply side*. Specifically, the researchers wanted to examine whether offering *matching grants*‚Äîwhere donations are matched by a lead donor‚Äîwould increase contributions.\n\n# ### Research Question\n\n# &gt; Does reducing the \"price\" of charitable giving through matching grants increase donations?\n\n# ---\n\n# ## Experimental Design and Methodology\n\n# The experiment was conducted in collaboration with a U.S.-based liberal nonprofit organization. The sample consisted of **50,083 prior donors**, randomly divided into two groups:\n\n# - A **control group** (~33%) received a standard four-page fundraising letter.\n# - A **treatment group** (~67%) received a similar letter with the inclusion of a **matching grant offer**.\n\n# ### Sub-Treatments within the Treatment Group\n\n# Recipients in the treatment group were further randomly assigned to one of several sub-conditions:\n\n# 1. **Matching Ratios**:\n#     - \\(1:1\\) ‚Äî For every \\$1 donated, the donor matched \\$1.\n#     - \\(2:1\\) ‚Äî For every \\$1 donated, the donor matched \\$2.\n#     - \\(3:1\\) ‚Äî For every \\$1 donated, the donor matched \\$3.\n\n# 2. **Maximum Match Amounts**:\n#     - \\$25,000  \n#     - \\$50,000  \n#     - \\$100,000  \n#     - Unspecified cap\n\n# 3. **Suggested Donation Examples** (based on past donation amounts):\n#     - 100% (same as highest previous contribution)\n#     - 125%\n#     - 150%\n\n# All letters were sent in **August 2005**, referencing a real-time political event (Supreme Court nominations), ensuring external validity through real financial stakes.\n\n# ---\n\n# ## Key Findings and Results\n\n# ### Matching Grant Effectiveness\n\n# - Simply **offering a match** increased:\n#   - **Response rate** by **22%**\n#   - **Revenue per solicitation** by **19%**\n\n# - **Higher match ratios** (\\(2:1\\), \\(3:1\\)) provided **no additional benefit** compared to \\(1:1\\).\n  \n# ### Behavioral Interpretations\n\n# - Matching offers likely served as a **signal of credibility and urgency**.\n# - Donors may have been influenced by **social cues** embedded in the match framing.\n\n# ### Other Insights\n\n# - **Cap size** and **suggested amount examples** showed no strong influence.\n# - **Political environment** mattered:\n#   - The **match offer increased donations significantly in red states** (+55% revenue per solicitation).\n#   - Minimal effect observed in blue states.\n  \n# ### Elasticity Estimate\n\n# - Estimated **price elasticity**:  \n#   \\[\n#   \\varepsilon \\approx -0.225\n#   \\]  \n#   This falls within the range of prior estimates derived from tax-based price effects.\n\n# ---\n\n# ## Implications\n\n# - Fundraisers **should use matching grants**, but **do not need to escalate the match ratio** for additional benefit.\n# - The experiment questions the assumption that *larger matches are always more persuasive*.\n# - Political and social context can significantly mediate the effectiveness of fundraising appeals.\n# - The results also inform broader public policy discussions around **cost-benefit analysis** and **valuation of public goods**.\n\n# ---\n\n# ## Publication and Data Access\n\n# The results of this study were published in:\n\n# &gt; **Karlan, Dean and John A. List (2007)**.  \n# &gt; \"*Does Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment*.\"  \n# &gt; *American Economic Review*, 97(5): 1774‚Äì1793.  \n# &gt; [https://doi.org/10.1257/aer.97.5.1774](https://doi.org/10.1257/aer.97.5.1774)\n\n# **Supporting data available at:**\n\n# - [AEA Data Archive](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774)\n# - [Innovations for Poverty Action (IPA)](https://www.poverty-action.org)\n# - [Harvard Dataverse](https://dataverse.harvard.edu)\n\n# ---\n# This project seeks to replicate their results.\n\n\nData\nDescription\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows √ó 51 columns\n\n\n\nBalance Test\n\nvariables_to_test = ['hpa', 'freq', 'years', 'female', 'red0']\nresults = {}\n\nfor var in variables_to_test:\n    treatment_data = df[df['treatment'] == 1][var].dropna()\n    control_data = df[df['control'] == 1][var].dropna()\n    \n    # T-test calculations\n    mean_diff = treatment_data.mean() - control_data.mean()\n    n_treat = len(treatment_data)\n    n_control = len(control_data)\n    var_treat = treatment_data.var(ddof=1)\n    var_control = control_data.var(ddof=1)\n    se_diff = ((var_treat / n_treat) + (var_control / n_control)) ** 0.5\n    t_stat = mean_diff / se_diff\n    df_denom = ((var_treat / n_treat)**2 / (n_treat - 1)) + ((var_control / n_control)**2 / (n_control - 1))\n    df_effective = (var_treat / n_treat + var_control / n_control)**2 / df_denom\n    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_effective))\n    \n    # Linear regression\n    y = df[var]\n    X = sm.add_constant(df['treatment'])\n    model = sm.OLS(y, X, missing='drop').fit()\n    reg_coef = model.params['treatment']\n    reg_pval = model.pvalues['treatment']\n    \n    results[var] = {\n        'T-test': {\n            'Mean (Treatment)': np.float64(round(treatment_data.mean(), 4)),\n            'Mean (Control)': np.float64(round(control_data.mean(), 4)),\n            't-statistic': np.float64(round(t_stat, 4)),\n            'df': np.float64(round(df_effective, 1)),\n            'p-value': np.float64(round(p_value, 4))\n        },\n        'Linear Regression': {\n            'coefficient': np.float64(round(reg_coef, 4)),\n            'p-value': np.float64(round(reg_pval, 4))\n        }\n    }\n\nresults\n\n{'hpa': {'T-test': {'Mean (Treatment)': np.float64(59.597198486328125),\n   'Mean (Control)': np.float64(58.960201263427734),\n   't-statistic': np.float64(0.9703999757766724),\n   'df': np.float64(35913.8984375),\n   'p-value': np.float64(0.3318)},\n  'Linear Regression': {'coefficient': np.float64(0.6371),\n   'p-value': np.float64(0.3451)}},\n 'freq': {'T-test': {'Mean (Treatment)': np.float64(8.0354),\n   'Mean (Control)': np.float64(8.0473),\n   't-statistic': np.float64(-0.1108),\n   'df': np.float64(33326.4),\n   'p-value': np.float64(0.9117)},\n  'Linear Regression': {'coefficient': np.float64(-0.012),\n   'p-value': np.float64(0.9117)}},\n 'years': {'T-test': {'Mean (Treatment)': np.float64(6.0784),\n   'Mean (Control)': np.float64(6.1359),\n   't-statistic': np.float64(-1.0909),\n   'df': np.float64(32400.9),\n   'p-value': np.float64(0.2753)},\n  'Linear Regression': {'coefficient': np.float64(-0.0575),\n   'p-value': np.float64(0.27)}},\n 'female': {'T-test': {'Mean (Treatment)': np.float64(0.2752),\n   'Mean (Control)': np.float64(0.2827),\n   't-statistic': np.float64(-1.7535),\n   'df': np.float64(32450.8),\n   'p-value': np.float64(0.0795)},\n  'Linear Regression': {'coefficient': np.float64(-0.0075),\n   'p-value': np.float64(0.0787)}},\n 'red0': {'T-test': {'Mean (Treatment)': np.float64(0.4074),\n   'Mean (Control)': np.float64(0.3986),\n   't-statistic': np.float64(1.8773),\n   'df': np.float64(33450.5),\n   'p-value': np.float64(0.0605)},\n  'Linear Regression': {'coefficient': np.float64(0.0087),\n   'p-value': np.float64(0.0608)}}}\n\n\nBalance Test and Comparison with Table 1\nTo validate the integrity of the randomization mechanism used in Karlan and List (2007), we perform a series of balance tests comparing the treatment and control groups on several pre-treatment variables. These include donor history, demographics, and political geography.\nVariables Tested\nWe test the following five variables:\n\n ‚Äì Highest previous contribution\n\n ‚Äì Number of prior donations\n\n ‚Äì Number of years since initial donation\n\n ‚Äì Indicator for female donor\n\n ‚Äì Indicator for living in a red (Bush 2004) state\n\nComparison with Table 1 from the Paper\nStatistical Test Results\nEach variable was tested for balance using both a two-sample t-test and a linear regression of the variable on the treatment indicator. All p-values exceeded 0.05, indicating no statistically significant differences between the treatment and control groups at the 95% confidence level.\nNotably:\n\n: p = 0.080 (t-test), p = 0.079 (regression)\n\n: p = 0.060 (t-test), p = 0.061 (regression)\n\nThese are close to significance at the 10% level but still support the assumption of balance.\nCommentary\nTable 1 is included in the original paper as a critical validation of the randomization strategy. Demonstrating statistical equivalence across treatment and control groups for observable characteristics helps establish that any observed treatment effects on outcomes are causally attributable to the experimental manipulation rather than pre-existing differences. Our balance test results confirm this, providing additional support for the credibility of the experimental design.\n\n\nExperimental Results\nCharitable Contribution Made\n\n# Step 1: Create binary donation variable\ndf['donated'] = df['gave'].fillna(0)\n\n# Step 2: Calculate donation proportions\ndonation_rates = df.groupby('treatment')['donated'].mean()\n\n# Plotting\nplt.figure(figsize=(6, 4))\nsns.barplot(x=donation_rates.index, y=donation_rates.values)\nplt.xticks([0, 1], ['Control', 'Treatment'])\nplt.ylabel('Proportion Who Donated')\nplt.title('Donation Rates by Treatment Group')\nplt.ylim(0, max(donation_rates.values) * 1.2)\nplt.tight_layout()\nplt.grid(axis='y')\nplt.show()\n\n# Step 3: T-test for donation rates\ntreatment_don = df[df['treatment'] == 1]['donated']\ncontrol_don = df[df['treatment'] == 0]['donated']\nt_stat, p_val = stats.ttest_ind(treatment_don, control_don, equal_var=False)\n\n# Step 4: Bivariate linear regression\nX_lin = sm.add_constant(df['treatment'])\nmodel_lin = sm.OLS(df['donated'], X_lin).fit()\n\n# Step 5: Probit regression\nmodel_probit = sm.Probit(df['donated'], X_lin).fit(disp=0)\n\n# Collect and return all results\n{\n    \"Donation Rates\": donation_rates.to_dict(),\n    \"T-test\": {\n        \"t_statistic\": t_stat,\n        \"p_value\": p_val\n    },\n    \"Linear Regression\": {\n        \"coefficient\": model_lin.params['treatment'],\n        \"p_value\": model_lin.pvalues['treatment'],\n        \"r_squared\": model_lin.rsquared\n    },\n    \"Probit Regression (Table 3 Col 1)\": {\n        \"coefficient\": model_probit.params['treatment'],\n        \"p_value\": model_probit.pvalues['treatment']\n    }\n}\n\n\n\n\n\n\n\n\n{'Donation Rates': {0: 0.017858212980164198, 1: 0.02203856749311295},\n 'T-test': {'t_statistic': np.float64(3.2094621908279835),\n  'p_value': np.float64(0.0013309823450914173)},\n 'Linear Regression': {'coefficient': np.float64(0.004180354512949392),\n  'p_value': np.float64(0.0019274025949017077),\n  'r_squared': np.float64(0.00019202078862623484)},\n 'Probit Regression (Table 3 Col 1)': {'coefficient': np.float64(0.08678462244745834),\n  'p_value': np.float64(0.0018523990147882028)}}\n\n\n\n# ## üéØ Analysis: Effect of Matched Donations on Charitable Giving\n\n# ### üìä Barplot ‚Äì Donation Rates by Group\n\n# The barplot shows:\n\n# \\begin{itemize}\n#   \\item \\textbf{Control Group:} Approximately 1.79\\% of individuals donated\n#   \\item \\textbf{Treatment Group:} Approximately 2.20\\% of individuals donated\n# \\end{itemize}\n\n# This simple visual confirms that a higher proportion of people donated when offered a matching grant.\n\n# ---\n\n# ### üî¨ Statistical Tests\n\n# #### ‚úÖ T-test\n\n# \\begin{itemize}\n#   \\item \\textbf{Result:} The difference in donation rates is statistically significant (\\( p = 0.0013 \\))\n#   \\item \\textbf{Interpretation:} There is strong evidence that the treatment group donated more than the control group, even after accounting for natural variation.\n# \\end{itemize}\n\n# ---\n\n# #### ‚úÖ Linear Regression\n\n# \\begin{itemize}\n#   \\item \\textbf{Model:} \\texttt{donated} $\\sim$ \\texttt{treatment\\_group}\n#   \\item \\textbf{Coefficient on treatment:} 0.0042 (i.e., a 0.42 percentage point increase in donation rate)\n#   \\item \\textbf{P-value:} 0.0019\n#   \\item \\textbf{Interpretation:} The regression confirms the same result ‚Äî a small but statistically significant increase in the likelihood of giving due to the matching grant.\n# \\end{itemize}\n\n# ---\n\n# #### ‚úÖ Probit Regression (Replicating Table 3, Column 1)\n\n# \\begin{itemize}\n#   \\item \\textbf{Coefficient on treatment:} 0.087\n#   \\item \\textbf{P-value:} 0.0019\n# \\end{itemize}\n\n# This closely matches Table 3, Column 1 in the original paper, where the treatment effect is also reported as statistically significant and positive.\n\n# ---\n\n# ### üß† Conclusion\n\n# These results suggest that offering a matching grant increases the probability of charitable giving. Even though the increase is modest, it is statistically significant and practically relevant for large-scale fundraising. This finding supports the notion that people are more likely to give when their donations are perceived to have more impact or when they feel socially encouraged to act.\n\n\n# ## Replication of Table 3, Column 1 (Probit Regression)\n\n# To assess whether our replication matches the findings in Karlan and List (2007), we estimated a probit regression where the dependent variable is a binary indicator for whether a charitable donation was made, and the explanatory variable is a treatment group indicator.\n\n# ### Model Specification\n# \\[\n# \\text{Pr}(\\text{donated}_i = 1) = \\Phi(\\alpha + \\beta \\cdot \\text{treatment}_i)\n# \\]\n\n# Where:\n# - \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution\n# - \\(\\text{treatment}_i = 1\\) if individual \\(i\\) received any matching grant offer\n# - \\(\\text{donated}_i = 1\\) if individual \\(i\\) made any charitable donation\n\n# ### Results\n\n# \\begin{table}[h!]\n# \\centering\n# \\begin{tabular}{lcc}\n# \\toprule\n# \\textbf{Variable} & \\textbf{Coefficient} & \\textbf{p-value} \\\\\n# \\midrule\n# Treatment Indicator & 0.087 & 0.0019 \\\\\n# \\bottomrule\n# \\end{tabular}\n# \\caption{Probit regression replicating Table 3, Column 1 of Karlan \\& List (2007)}\n# \\end{table}\n\n# ### Interpretation\n\n# Our estimated coefficient of \\(\\beta = 0.087\\) and associated p-value closely match the published results in Table 3, Column 1 of the original paper. This confirms the authors‚Äô key finding: assignment to the treatment group (receiving a matching grant offer) significantly increased the likelihood of making a donation.\n\n# The positive and statistically significant coefficient implies that simply offering a matching gift makes people more likely to give, even if the size of the match or other conditions vary. This supports the idea that social framing and perceived impact can meaningfully influence charitable behavior.\n\nDifferences between Match Rates\n\n# Filter to treatment group only\ntreatment_df = df[df['treatment'] == 1].copy()\n\n# Create ratio dummies\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\n# Response rates for each ratio group\nresponse_rates = treatment_df.groupby('ratio')['donated'].mean()\n\n# T-tests\nt1_vs_t2 = stats.ttest_ind(\n    treatment_df[treatment_df['ratio'] == 1]['donated'],\n    treatment_df[treatment_df['ratio'] == 2]['donated'],\n    equal_var=False\n)\n\nt2_vs_t3 = stats.ttest_ind(\n    treatment_df[treatment_df['ratio'] == 2]['donated'],\n    treatment_df[treatment_df['ratio'] == 3]['donated'],\n    equal_var=False\n)\n\n# Regression on ratio dummies (no intercept to make each coefficient a group mean)\nX_ratios = treatment_df[['ratio1', 'ratio2', 'ratio3']]\nmodel_ratios = sm.OLS(treatment_df['donated'], X_ratios).fit()\n\n# Compute differences from regression coefficients\ncoef_1v2 = model_ratios.params['ratio2'] - model_ratios.params['ratio1']\ncoef_2v3 = model_ratios.params['ratio3'] - model_ratios.params['ratio2']\n\n# Get standard errors of differences using variance-covariance matrix\nvcov = model_ratios.cov_params()\nse_1v2 = (vcov.loc['ratio1', 'ratio1'] + vcov.loc['ratio2', 'ratio2'] - 2 * vcov.loc['ratio1', 'ratio2']) ** 0.5\nse_2v3 = (vcov.loc['ratio2', 'ratio2'] + vcov.loc['ratio3', 'ratio3'] - 2 * vcov.loc['ratio2', 'ratio3']) ** 0.5\n\n# Return all results\n{\n    \"Response Rates\": response_rates.to_dict(),\n    \"T-test 1:1 vs 2:1\": {\n        \"t_statistic\": t1_vs_t2.statistic,\n        \"p_value\": t1_vs_t2.pvalue\n    },\n    \"T-test 2:1 vs 3:1\": {\n        \"t_statistic\": t2_vs_t3.statistic,\n        \"p_value\": t2_vs_t3.pvalue\n    },\n    \"Regression Coefficients\": model_ratios.params.to_dict(),\n    \"Regression P-values\": model_ratios.pvalues.to_dict(),\n    \"Coef Diff 1:1 vs 2:1\": {\n        \"difference\": coef_1v2,\n        \"std_error\": se_1v2\n    },\n    \"Coef Diff 2:1 vs 3:1\": {\n        \"difference\": coef_2v3,\n        \"std_error\": se_2v3\n    }\n}\n\n/var/folders/69/xm9kgxts537cl3n81zl1z2mw0000gn/T/ipykernel_45911/2487579542.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  response_rates = treatment_df.groupby('ratio')['donated'].mean()\n\n\n{'Response Rates': {'Control': nan,\n  1: 0.020749124225276205,\n  2: 0.0226333752469912,\n  3: 0.022733399227244138},\n 'T-test 1:1 vs 2:1': {'t_statistic': np.float64(-0.965048975142932),\n  'p_value': np.float64(0.33453078237183076)},\n 'T-test 2:1 vs 3:1': {'t_statistic': np.float64(-0.05011581369764474),\n  'p_value': np.float64(0.9600305476940865)},\n 'Regression Coefficients': {'ratio1': 0.020749124225276205,\n  'ratio2': 0.02263337524699119,\n  'ratio3': 0.022733399227244135},\n 'Regression P-values': {'ratio1': 3.981332654481922e-50,\n  'ratio2': 2.855649540252303e-59,\n  'ratio3': 9.435357513198687e-60},\n 'Coef Diff 1:1 vs 2:1': {'difference': np.float64(0.001884251021714984),\n  'std_error': np.float64(0.001967717735984604)},\n 'Coef Diff 2:1 vs 3:1': {'difference': np.float64(0.00010002398025294595),\n  'std_error': np.float64(0.0019678945459136407)}}\n\n\n\n# ## üéØ Effectiveness of Different Match Ratios on Donation Rates\n\n# ### üìä Response Rates by Match Ratio\n\n# \\begin{itemize}\n#   \\item \\textbf{1:1 Match:} 2.07\\%\n#   \\item \\textbf{2:1 Match:} 2.26\\%\n#   \\item \\textbf{3:1 Match:} 2.27\\%\n# \\end{itemize}\n\n# These response rates are very close to each other, with only slight increases as the match ratio increases.\n\n# ---\n\n# ### üî¨ Statistical Tests\n\n# #### ‚úÖ T-tests\n\n# \\begin{itemize}\n#   \\item \\textbf{1:1 vs. 2:1:} Not statistically significant ($p = 0.335$)\n#   \\item \\textbf{2:1 vs. 3:1:} Not statistically significant ($p = 0.960$)\n# \\end{itemize}\n\n# These results do \\textbf{not} provide evidence that higher match ratios significantly increase the likelihood of donating.\n\n# ---\n\n# ### üìà Regression Results\n\n# Ordinary least squares (OLS) regression of \\texttt{donated} on dummy indicators for match ratios (no intercept, so each coefficient equals the group mean):\n\n# \\begin{table}[h!]\n# \\centering\n# \\begin{tabular}{lcc}\n# \\toprule\n# \\textbf{Match Ratio} & \\textbf{Coefficient (Mean Response Rate)} & \\textbf{p-value} \\\\\n# \\midrule\n# 1:1 & 0.02075 & &lt; 0.0001 \\\\\n# 2:1 & 0.02263 & &lt; 0.0001 \\\\\n# 3:1 & 0.02273 & &lt; 0.0001 \\\\\n# \\bottomrule\n# \\end{tabular}\n# \\caption{Regression coefficients for response rates by match ratio}\n# \\end{table}\n\n# ---\n\n# ### üîç Difference in Response Rates from Regression Coefficients\n\n# \\begin{itemize}\n#   \\item \\textbf{2:1 - 1:1:} +0.00188 (0.19 percentage points), SE = 0.00197\n#   \\item \\textbf{3:1 - 2:1:} +0.00010 (0.01 percentage points), SE = 0.00197\n# \\end{itemize}\n\n# Both differences are \\textbf{not statistically significant}.\n\n# ---\n\n# ### üß† Interpretation\n\n# These findings are consistent with the authors‚Äô comment on \\textbf{page 8} of the paper. While any match appears to increase giving, offering a larger match (e.g., 2:1 or 3:1 instead of 1:1) does \\textbf{not significantly increase the likelihood of donating}.\n\n# This suggests that the psychological effect of a match---such as signaling importance or social endorsement---may be more influential than the actual financial leverage provided. Donors may respond to the presence of a match offer, rather than its size.\n\nSize of Charitable Contribution\n\n# Step 1: OLS regression on full sample\nX_full = sm.add_constant(df['treatment'])\nmodel_full = sm.OLS(df['amount'], X_full, missing='drop').fit()\n\n# Step 2: OLS regression on donors only\ndonors_df = df[df['amount'] &gt; 0].copy()\nX_donors = sm.add_constant(donors_df['treatment'])\nmodel_donors = sm.OLS(donors_df['amount'], X_donors).fit()\n\n# Step 3: Prepare histograms for donors\ntreatment_donors = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_donors = donors_df[donors_df['treatment'] == 0]['amount']\n\n# Plot: Control group\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, edgecolor='black')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Donation Amounts: Control Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\n# Plot: Treatment group\nplt.subplot(1, 2, 2)\nplt.hist(treatment_donors, bins=30, alpha=0.7, edgecolor='black')\nplt.axvline(treatment_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Donation Amounts: Treatment Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# Return regression summaries\n{\n    \"OLS Full Sample\": {\n        \"coefficient\": model_full.params['treatment'],\n        \"p_value\": model_full.pvalues['treatment']\n    },\n    \"OLS Donors Only\": {\n        \"coefficient\": model_donors.params['treatment'],\n        \"p_value\": model_donors.pvalues['treatment']\n    }\n}\n\n\n\n\n\n\n\n\n{'OLS Full Sample': {'coefficient': np.float64(0.1536054649919702),\n  'p_value': np.float64(0.0628202949210884)},\n 'OLS Donors Only': {'coefficient': np.float64(-1.6683934553392632),\n  'p_value': np.float64(0.5614755766155084)}}\n\n\n\n# ## üíµ Analysis: Effect of Match Offer on Donation Amount\n\n# ---\n\n# ### üî¨ OLS Regression ‚Äì Full Sample\n\n# We estimate the following model on the full sample (including those who gave \\$0):\n\n# \\[\n# \\text{amount}_i = \\alpha + \\beta \\cdot \\text{treatment}_i + \\varepsilon_i\n# \\]\n\n# \\begin{itemize}\n#   \\item \\textbf{Estimated Coefficient on Treatment:} $\\beta = 0.154$\n#   \\item \\textbf{p-value:} 0.063\n# \\end{itemize}\n\n# \\textbf{Interpretation:} Individuals in the treatment group gave on average 15 cents more than those in the control group. While this difference is not statistically significant at the 5\\% level, it is marginally significant at the 10\\% level, suggesting that matching grants may modestly increase overall donations by encouraging more people to give.\n\n# ---\n\n# ### üî¨ OLS Regression ‚Äì Donors Only\n\n# We restrict the sample to individuals who made a positive donation and estimate the same model:\n\n# \\[\n# \\text{amount}_i = \\alpha + \\beta \\cdot \\text{treatment}_i + \\varepsilon_i \\quad \\text{where} \\quad \\text{amount}_i &gt; 0\n# \\]\n\n# \\begin{itemize}\n#   \\item \\textbf{Estimated Coefficient on Treatment:} $\\beta = -1.67$\n#   \\item \\textbf{p-value:} 0.561\n# \\end{itemize}\n\n# \\textbf{Interpretation:} Among individuals who donated, those in the treatment group gave \\$1.67 less on average than those in the control group. However, this result is not statistically significant, implying that the match offer had little to no effect on the \\emph{amount} donated once someone decided to give.\n\n# ---\n\n# ### üìä Histograms of Donation Amounts (Donors Only)\n\n# We visualize the distribution of donation amounts among donors for both the treatment and control groups. Each plot includes a red dashed line indicating the group-specific mean donation.\n\n# \\begin{itemize}\n#   \\item Both distributions are highly right-skewed, which is typical in donation data.\n#   \\item The visual difference in average donation between groups is minimal.\n# \\end{itemize}\n\n# ---\n\n# ### üß† Conclusion\n\n# This analysis suggests that matching grants may increase the \\emph{likelihood} of giving, but not the \\emph{amount} given among those who choose to donate. Therefore, the treatment coefficient in the donors-only regression does not carry a clear causal interpretation‚Äîit may be driven by selection into giving rather than by the treatment itself.\n\n\n\nSimulation Experiment\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_simulations = 10000\nsample_sizes = [50, 100, 500, 1000]\n\n# Container for simulation results\nsim_results = {}\n\n# Simulate for each sample size\nfor n in sample_sizes:\n    # Simulate many samples for control and treatment\n    control_samples = np.random.binomial(1, p_control, size=(n_simulations, n))\n    treatment_samples = np.random.binomial(1, p_treatment, size=(n_simulations, n))\n\n    # Calculate sample means\n    control_means = control_samples.mean(axis=1)\n    treatment_means = treatment_samples.mean(axis=1)\n\n    # Calculate sample t-statistics\n    pooled_std = np.sqrt((control_means * (1 - control_means) / n) +\n                         (treatment_means * (1 - treatment_means) / n))\n    t_stats = (treatment_means - control_means) / pooled_std\n\n    # Save results\n    sim_results[n] = {\n        \"mean_control\": np.mean(control_means),\n        \"mean_treatment\": np.mean(treatment_means),\n        \"mean_t_statistic\": np.mean(t_stats),\n        \"std_t_statistic\": np.std(t_stats),\n        \"t_statistics\": t_stats\n    }\n\nsim_results_summary = {\n    n: {\n        \"Mean (Control)\": res[\"mean_control\"],\n        \"Mean (Treatment)\": res[\"mean_treatment\"],\n        \"Mean t-stat\": res[\"mean_t_statistic\"],\n        \"SD t-stat\": res[\"std_t_statistic\"]\n    }\n    for n, res in sim_results.items()\n}\n\nsim_results_summary\n\n/var/folders/69/xm9kgxts537cl3n81zl1z2mw0000gn/T/ipykernel_45911/145386153.py:26: RuntimeWarning: invalid value encountered in divide\n  t_stats = (treatment_means - control_means) / pooled_std\n\n\n{50: {'Mean (Control)': np.float64(0.017974),\n  'Mean (Treatment)': np.float64(0.022302000000000002),\n  'Mean t-stat': np.float64(nan),\n  'SD t-stat': np.float64(nan)},\n 100: {'Mean (Control)': np.float64(0.017918),\n  'Mean (Treatment)': np.float64(0.022043),\n  'Mean t-stat': np.float64(nan),\n  'SD t-stat': np.float64(nan)},\n 500: {'Mean (Control)': np.float64(0.0179908),\n  'Mean (Treatment)': np.float64(0.022036000000000004),\n  'Mean t-stat': np.float64(0.45369191126021896),\n  'SD t-stat': np.float64(1.003004605459035)},\n 1000: {'Mean (Control)': np.float64(0.018000299999999997),\n  'Mean (Treatment)': np.float64(0.0219458),\n  'Mean t-stat': np.float64(0.6298853379353075),\n  'SD t-stat': np.float64(0.9899356632471313)}}\n\n\n\n# Plot histograms of t-statistics for sample sizes 500 and 1000\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nfor i, n in enumerate([500, 1000]):\n    t_stats = sim_results[n]['t_statistics']\n    axs[i].hist(t_stats, bins=50, alpha=0.7, edgecolor='black', density=True)\n    axs[i].axvline(x=np.mean(t_stats), color='red', linestyle='--', label='Mean t-stat')\n    axs[i].set_title(f'T-statistics Distribution (n = {n})')\n    axs[i].set_xlabel('t-statistic')\n    axs[i].set_ylabel('Density')\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# ## üé≤ Simulation Demonstration: Law of Large Numbers and Central Limit Theorem\n\n# We simulate data for two groups based on the experimental setup:\n\n# \\begin{itemize}\n#   \\item Control group donation behavior is modeled as a Bernoulli distribution with $p = 0.018$\n#   \\item Treatment group donation behavior is modeled as a Bernoulli distribution with $p = 0.022$\n# \\end{itemize}\n\n# For each sample size, we simulate 10{,}000 draws to compute sample means and t-statistics comparing the two groups.\n\n# ---\n\n# ### üìä Simulation Summary\n\n# \\begin{table}[h!]\n# \\centering\n# \\begin{tabular}{ccccc}\n# \\toprule\n# \\textbf{Sample Size} & \\textbf{Mean (Control)} & \\textbf{Mean (Treatment)} & \\textbf{Mean t-stat} & \\textbf{SD t-stat} \\\\\n# \\midrule\n# 50   & 0.01797 & 0.02230 & NaN  & NaN \\\\\n# 100  & 0.01792 & 0.02204 & NaN  & NaN \\\\\n# 500  & 0.01799 & 0.02204 & 0.454 & 1.003 \\\\\n# 1000 & 0.01800 & 0.02195 & 0.630 & 0.990 \\\\\n# \\bottomrule\n# \\end{tabular}\n# \\caption{Simulated sample statistics and t-statistics across increasing sample sizes}\n# \\end{table}\n\n# ---\n\n# ### üìà Visualization\n\n# The following histograms show the distribution of t-statistics for sample sizes $n = 500$ and $n = 1000$. The red dashed line marks the mean of the t-statistics.\n\n# - As sample size increases, the t-statistic distribution becomes approximately normal.\n# - The standard deviation of t-statistics approaches 1, consistent with the Central Limit Theorem.\n\n# ---\n\n# ### üß† Interpretation\n\n# \\begin{itemize}\n#   \\item The \\textbf{Law of Large Numbers} is evident in how the sample means converge to the true probabilities (0.018 and 0.022).\n#   \\item The \\textbf{Central Limit Theorem} is demonstrated by the near-normal distribution of t-statistics as $n$ increases.\n#   \\item At small sample sizes (e.g., $n = 50$), the t-statistics become unstable due to low variance in Bernoulli outcomes, leading to undefined or highly variable results.\n# \\end{itemize}\n\n# This simulation reinforces the reliability of inference tools like the t-test in large samples and helps explain why small sample sizes can produce misleading or imprecise statistical conclusions.\n\nLaw of Large Numbers\n\n# Simulate large number of draws\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(1, 0.018, size=100000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\n# Take the first 10,000 from control to match length\ncontrol_draws_subset = control_draws[:10000]\n\n# Compute vector of differences\ndiff_vector = treatment_draws - control_draws_subset\n\n# Compute cumulative average of differences\ncumulative_avg_diff = np.cumsum(diff_vector) / np.arange(1, len(diff_vector) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_avg_diff, label='Cumulative Average of Differences')\nplt.axhline(y=0.022 - 0.018, color='red', linestyle='--', label='True Mean Difference (0.004)')\nplt.title('Law of Large Numbers: Cumulative Average of Differences')\nplt.xlabel('Number of Simulated Observations')\nplt.ylabel('Cumulative Average')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# ## üìà Law of Large Numbers Demonstration\n\n# To illustrate the Law of Large Numbers (LLN), we simulate the following:\n\n# \\begin{itemize}\n#   \\item 100,000 observations from the control group distribution: Bernoulli($p=0.018$)\n#   \\item 10,000 observations from the treatment group distribution: Bernoulli($p=0.022$)\n# \\end{itemize}\n\n# We then compute a vector of 10,000 differences between matched treatment and control observations, and calculate the \\textbf{cumulative average} of these differences. The result is plotted below:\n\n# ---\n\n# ### üîç Plot Description\n\n# - The line plot displays the cumulative average of the difference between treatment and control outcomes as the number of observations increases.\n# - The red dashed line represents the \\textbf{true difference in population means}:\n# \\[\n# \\mathbb{E}[\\text{Treatment}] - \\mathbb{E}[\\text{Control}] = 0.022 - 0.018 = 0.004\n# \\]\n\n# ---\n\n# ### üß† Interpretation\n\n# The plot demonstrates that:\n\n# \\begin{itemize}\n#   \\item Early in the simulation, the cumulative average fluctuates due to the small sample size and randomness.\n#   \\item As more observations accumulate, the sample average \\textbf{stabilizes and converges} toward the true population mean difference.\n# \\end{itemize}\n\n# This behavior is consistent with the \\textbf{Law of Large Numbers}, which states that the average of a large number of independent and identically distributed random variables converges in probability to the expected value.\n\n# \\textbf{Conclusion:} The cumulative average in our simulation closely approaches the true mean difference of 0.004, providing a visual and statistical demonstration of the LLN in action.\n\nCentral Limit Theorem\n\n# Set up parameters for the CLT demonstration\nsample_sizes = [50, 200, 500, 1000]\nn_replications = 1000\np_control = 0.018\np_treatment = 0.022\n\n# Container for results\nmean_diffs_by_n = {}\n\n# Generate 4 histograms for the sample sizes\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(n_replications):\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n        mean_diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(mean_diff)\n    \n    mean_diffs_by_n[n] = mean_diffs\n    \n    ax = axs[i // 2, i % 2]\n    ax.hist(mean_diffs, bins=30, edgecolor='black', alpha=0.7, density=True)\n    ax.axvline(x=np.mean(mean_diffs), color='red', linestyle='--', label='Mean Difference')\n    ax.axvline(x=0, color='black', linestyle=':', label='Zero')\n    ax.set_title(f'Sample Size = {n}')\n    ax.set_xlabel('Average Difference (Treatment - Control)')\n    ax.set_ylabel('Density')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# ## üìà Central Limit Theorem Demonstration\n\n# To illustrate the Central Limit Theorem (CLT), we simulate the following experiment for four different sample sizes: $n = 50$, $200$, $500$, and $1000$.\n\n# ### üî¨ Simulation Procedure\n\n# For each sample size $n$, we:\n\n# \\begin{enumerate}\n#   \\item Draw $n$ samples from the control group: Bernoulli($p = 0.018$)\n#   \\item Draw $n$ samples from the treatment group: Bernoulli($p = 0.022$)\n#   \\item Compute the difference in sample means (treatment minus control)\n#   \\item Repeat steps 1--3 a total of 1,000 times\n# \\end{enumerate}\n\n# We then plot the histogram of the 1,000 average differences for each sample size.\n\n# ---\n\n# ### üìä Interpretation of Histograms\n\n# \\begin{itemize}\n#   \\item \\textbf{Sample Size = 50:}  \n#   The distribution is jagged and variable. Zero is relatively close to the center of the distribution due to high variance in small samples.\n\n#   \\item \\textbf{Sample Size = 200:}  \n#   The distribution becomes smoother and more symmetric. The sampling mean approaches the true mean difference of $0.004$.\n\n#   \\item \\textbf{Sample Size = 500 and 1000:}  \n#   The distribution is bell-shaped and closely resembles a normal distribution. The value zero is clearly in the tail, and the mean difference centers tightly around $0.004$.\n# \\end{itemize}\n\n# ---\n\n# ### üß† Conclusion\n\n# This set of simulations provides a clear visual demonstration of the \\textbf{Central Limit Theorem}:\n\n# \\begin{itemize}\n#   \\item As sample size increases, the distribution of average differences becomes increasingly \\textbf{normal}.\n#   \\item The \\textbf{sample mean converges} to the true population difference.\n#   \\item At large sample sizes, the null value of zero is in the tail of the distribution, indicating that we would likely \\textbf{reject the null hypothesis} of no treatment effect.\n# \\end{itemize}\n\n# The CLT explains why even binary outcomes, when averaged in large samples, yield inference procedures (like the t-test) that rely on normal approximations."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#context-and-purpose",
    "href": "blog/hw1/hw1_questions.html#context-and-purpose",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Context and Purpose",
    "text": "Context and Purpose\nIn the world of charitable fundraising, organizations often face a core challenge: how to effectively encourage people to donate. While private giving in the U.S. has historically been robust‚Äîoften exceeding 2% of GDP‚Äîfundraisers have long relied on anecdotes and rules of thumb rather than scientific evidence to guide their strategies.\nTo bring empirical clarity to this issue, economists Dean Karlan (Yale University) and John List (University of Chicago) conducted a landmark field experiment to investigate whether and how the ‚Äúprice‚Äù of giving affects charitable donations.\nThe study focused on understanding the demand side of charitable giving, which is less studied compared to the tax-incentivized supply side. Specifically, the researchers wanted to examine whether offering matching grants‚Äîwhere donations are matched by a lead donor‚Äîwould increase contributions.\n\nResearch Question\n\nDoes reducing the ‚Äúprice‚Äù of charitable giving through matching grants increase donations?"
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#experimental-design-and-methodology",
    "href": "blog/hw1/hw1_questions.html#experimental-design-and-methodology",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Design and Methodology",
    "text": "Experimental Design and Methodology\nThe experiment was conducted in collaboration with a U.S.-based liberal nonprofit organization. The sample consisted of 50,083 prior donors, randomly divided into two groups:\n\nA control group (~33%) received a standard four-page fundraising letter.\nA treatment group (~67%) received a similar letter with the inclusion of a matching grant offer.\n\n\nSub-Treatments within the Treatment Group\nRecipients in the treatment group were further randomly assigned to one of several sub-conditions:\n\nMatching Ratios:\n\n(1:1) ‚Äî For every $1 donated, the donor matched $1.\n(2:1) ‚Äî For every $1 donated, the donor matched $2.\n(3:1) ‚Äî For every $1 donated, the donor matched $3.\n\nMaximum Match Amounts:\n\n$25,000\n\n$50,000\n\n$100,000\n\nUnspecified cap\n\nSuggested Donation Examples (based on past donation amounts):\n\n100% (same as highest previous contribution)\n125%\n150%\n\n\nAll letters were sent in August 2005, referencing a real-time political event (Supreme Court nominations), ensuring external validity through real financial stakes."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#key-findings-and-results",
    "href": "blog/hw1/hw1_questions.html#key-findings-and-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Key Findings and Results",
    "text": "Key Findings and Results\n\nMatching Grant Effectiveness\n\nSimply offering a match increased:\n\nResponse rate by 22%\nRevenue per solicitation by 19%\n\nHigher match ratios ((2:1), (3:1)) provided no additional benefit compared to (1:1).\n\n\n\nBehavioral Interpretations\n\nMatching offers likely served as a signal of credibility and urgency.\nDonors may have been influenced by social cues embedded in the match framing.\n\n\n\nOther Insights\n\nCap size and suggested amount examples showed no strong influence.\nPolitical environment mattered:\n\nThe match offer increased donations significantly in red states (+55% revenue per solicitation).\nMinimal effect observed in blue states.\n\n\n\n\nElasticity Estimate\n\nEstimated price elasticity:\n[ ]\nThis falls within the range of prior estimates derived from tax-based price effects."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#implications",
    "href": "blog/hw1/hw1_questions.html#implications",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Implications",
    "text": "Implications\n\nFundraisers should use matching grants, but do not need to escalate the match ratio for additional benefit.\nThe experiment questions the assumption that larger matches are always more persuasive.\nPolitical and social context can significantly mediate the effectiveness of fundraising appeals.\nThe results also inform broader public policy discussions around cost-benefit analysis and valuation of public goods."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#publication-and-data-access",
    "href": "blog/hw1/hw1_questions.html#publication-and-data-access",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Publication and Data Access",
    "text": "Publication and Data Access\nThe results of this study were published in:\n\nKarlan, Dean and John A. List (2007).\n‚ÄúDoes Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment.‚Äù\nAmerican Economic Review, 97(5): 1774‚Äì1793.\nhttps://doi.org/10.1257/aer.97.5.1774\n\nSupporting data available at:\n\nAEA Data Archive\nInnovations for Poverty Action (IPA)\nHarvard Dataverse\n\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html",
    "href": "blog/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln \nimport statsmodels.api as sm\n\nairbnb_df = pd.read_csv('airbnb.csv')\nblueprinty_df = pd.read_csv('blueprinty.csv')\n\nFirms using Blueprinty‚Äôs software (customers) typically have more patents on average (around 4.13 patents) compared to non-customers (about 3.47 patents). The histograms reveal that both groups have distributions skewed toward lower numbers of patents, but Blueprinty customers generally have a higher frequency of firms with a larger number of patents. This preliminary analysis suggests a positive association between using Blueprinty‚Äôs software and obtaining patents.\n\nplt.figure(figsize=(10, 5))\nblueprinty_df[blueprinty_df['iscustomer'] == 0]['patents'].hist(alpha=0.5, label='Non-customer', bins=20)\nblueprinty_df[blueprinty_df['iscustomer'] == 1]['patents'].hist(alpha=0.5, label='Customer', bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\nmean_patents = blueprinty_df.groupby('iscustomer')['patents'].mean()\nprint(mean_patents)\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nCustomers and non-customers differ in regional representation: customers are predominantly located in the Northeast, whereas non-customers are more evenly spread across various regions. Regarding age, there‚Äôs little practical difference between the two groups, although customers tend to be slightly older on average (approximately 26.9 years) compared to non-customers (approximately 26.1 years). This indicates regional factors may play a role in the decision to adopt Blueprinty‚Äôs software, while age differences are relatively minor.\n\nregion_counts = pd.crosstab(blueprinty_df['region'], blueprinty_df['iscustomer'])\nregion_counts.plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count of Firms\")\nplt.legend(title='Customer Status', labels=['Non-customer', 'Customer'])\nplt.show()\n\nblueprinty_df.boxplot(column='age', by='iscustomer', figsize=(8,5))\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Firm Age\")\nplt.suptitle('')\nplt.show()\n\nprint(blueprinty_df.groupby('iscustomer')['age'].mean())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the probability density function is:\n¬†\\(f(|\\lambda) = \\frac{e^{-\\lambda}\\lambda^{Y}}{Y!}\\)\n\nThus, the likelihood function for the observed data \\((Y_1, Y_2, \\dots, Y_n)\\) is: ¬†\\(L(\\lambda|Y) = \\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\lambda^{Y_i}}{Y_i!}\\)\n\nAnd the corresponding log-likelihood function is: ¬†\\(\\ell(\\lambda|Y) = \\sum_{i=1}^{n}\\left[Y_i\\log(\\lambda) - \\lambda - \\log(Y_i!)\\right]\\)\n\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Calculate the Poisson log-likelihood for given lambda and observed Y.\n    \n    Parameters:\n    lam (float): Poisson parameter lambda (&gt; 0)\n    Y (array-like): Observed counts\n    \n    Returns:\n    float: Log-likelihood value\n    \"\"\"\n    if lam &lt;= 0:\n        return -np.inf\n    \n    log_factorials = np.array([np.sum(np.log(np.arange(1, y+1))) if y &gt; 0 else 0 for y in Y])\n    log_likelihood = np.sum(Y * np.log(lam) - lam - log_factorials)\n    \n    return log_likelihood\n\n\nY = blueprinty_df['patents'].values\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nloglike_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglike_values, label='Log-Likelihood')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood across Lambda Values')\nplt.axvline(lambda_values[np.argmax(loglike_values)], color='r', linestyle='--', label='Max Log-Likelihood')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStarting from the log-likelihood function:\n¬†\\(\\ell(\\lambda|Y) = \\sum_{i=1}^{n}\\left[Y_i\\log(\\lambda) - \\lambda - \\log(Y_i!)\\right]\\)\n\nTake the first derivative with respect to \\((\\lambda\\)\\):\n¬†\\(\\frac{d\\ell(\\lambda|Y)}{d\\lambda} = \\sum_{i=1}^{n}\\left(\\frac{Y_i}{\\lambda} - 1\\right)\\)\n\nSet this derivative equal to zero to find the maximum likelihood estimate (MLE):\n¬†\\(\\sum_{i=1}^{n}\\left(\\frac{Y_i}{\\lambda} - 1\\right) = 0\\)\n\nSolve for \\((\\lambda\\)\\):\n¬†\\(\\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} - n = 0 \\quad \\Longrightarrow \\quad \\lambda = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\bar{Y}\\)\n\nThus, the maximum likelihood estimate for \\((\\lambda)\\) is simply the sample mean, \\((\\bar{Y})\\), which intuitively makes sense since the mean of a Poisson distribution is \\((\\lambda)\\).\n\nY = blueprinty_df['patents'].values\n\ndef neg_poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lam) - lam)\n\ninitial_guess = np.mean(Y)\n\nresult = minimize(neg_poisson_loglikelihood, \n                  x0=[initial_guess], \n                  args=(Y,),\n                  bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nprint(\"MLE for lambda:\", lambda_mle)\n\nMLE for lambda: 3.6846666666676033\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X_beta = X @ beta\n    X_beta = np.clip(X_beta, -20, 20)  # prevent overflow in exp()\n    lambda_i = np.exp(X_beta)\n    log_factorials = gammaln(Y + 1)  # stable log-factorial\n    loglik = np.sum(Y * X_beta - lambda_i - log_factorials)\n    return -loglik\n\n\nblueprinty_df['age_squared'] = blueprinty_df['age'] ** 2\nscaler = StandardScaler()\nblueprinty_df[['age', 'age_squared']] = scaler.fit_transform(blueprinty_df[['age', 'age_squared']])\n\nX = pd.get_dummies(blueprinty_df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX.insert(0, 'intercept', 1)\nX_matrix = X.to_numpy(dtype=np.float64)\nY_vector = blueprinty_df['patents'].to_numpy(dtype=np.int32)\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta)\n    X_beta = X @ beta\n    X_beta = np.clip(X_beta, -20, 20)  \n    lambda_i = np.exp(X_beta)\n    log_factorials = gammaln(Y + 1)  \n    loglik = np.sum(Y * X_beta - lambda_i - log_factorials)\n    return -loglik \n\nbeta_start = np.zeros(X_matrix.shape[1])\nregression_result = minimize(\n    poisson_regression_loglikelihood,\n    beta_start,\n    args=(Y_vector, X_matrix),\n    method='BFGS'\n)\n\nbeta_hat = regression_result.x\nhessian_inv = regression_result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n                  Coefficient  Std. Error\nintercept            1.188966    0.038775\nage                  1.076022    0.115656\nage_squared         -1.181549    0.189616\niscustomer           0.207591    0.031799\nregion_Northeast     0.029170    0.064828\nregion_Northwest    -0.017574    0.070656\nregion_South         0.056561    0.058948\nregion_Southwest     0.050576    0.067469\n\n\n\nX_clean = X.drop(columns='intercept', errors='ignore').astype(float)\nX_glm = sm.add_constant(X_clean)\nY_numeric = Y_vector.astype(int)\n\nglm_model = sm.GLM(Y_numeric, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\ncustomer_col = [col for col in glm_results.model.exog_names if 'iscustomer' in col][0]\n\nX1_glm = X_glm.copy()\nX0_glm = X_glm.copy()\nX1_glm[customer_col] = 1\nX0_glm[customer_col] = 0\n\nX1_glm = X1_glm[glm_results.model.exog_names]\nX0_glm = X0_glm[glm_results.model.exog_names]\n\ny_pred_1 = glm_results.predict(X1_glm)\ny_pred_0 = glm_results.predict(X0_glm)\naverage_treatment_effect = np.mean(y_pred_1 - y_pred_0)\n\nfrom tabulate import tabulate\n\ncoef_df = glm_results.summary2().tables[1].reset_index()\ncoef_df.columns = ['Variable', 'Coefficient', 'Std. Error', 'z', 'P&gt;|z|', 'CI Lower', 'CI Upper']\nsummary_table = tabulate(coef_df, headers=\"keys\", tablefmt=\"github\", showindex=False)\n\nlog_likelihood = glm_results.llf\npseudo_r2 = 1 - glm_results.deviance / glm_results.null_deviance\nn_obs = glm_results.nobs\n\nfinal_summary = f\"\"\"\n###  Poisson Regression Results (`statsmodels.GLM`)\n\n{summary_table}\n\n- **Log-Likelihood**: {log_likelihood:.1f}  \n- **Pseudo R¬≤ (Cox & Snell)**: {pseudo_r2:.3f}  \n- **Number of Observations**: {int(n_obs)}  \n- **Model Family**: Poisson (log link)\n\n---\n\n### Estimated Effect of Blueprinty's Software\n\n- **Average Treatment Effect (Predicted)**: **{average_treatment_effect:.4f} patents**\n\"\"\"\nprint(final_summary)\n\n\n###  Poisson Regression Results (`statsmodels.GLM`)\n\n| Variable         |   Coefficient |   Std. Error |          z |        P&gt;|z| |   CI Lower |   CI Upper |\n|------------------|---------------|--------------|------------|--------------|------------|------------|\n| const            |     1.18897   |    0.0367361 |  32.3651   | 8.51588e-230 |  1.11696   |  1.26097   |\n| age              |     1.07602   |    0.10041   |  10.7162   | 8.5396e-27   |  0.879221  |  1.27282   |\n| age_squared      |    -1.18155   |    0.102625  | -11.5132   | 1.1315e-30   | -1.38269   | -0.980407  |\n| iscustomer       |     0.207591  |    0.0308953 |   6.71918  | 1.82751e-11  |  0.147037  |  0.268144  |\n| region_Northeast |     0.0291701 |    0.0436255 |   0.668647 | 0.50372      | -0.0563343 |  0.114674  |\n| region_Northwest |    -0.0175745 |    0.0537806 |  -0.326782 | 0.743833     | -0.122983  |  0.0878335 |\n| region_South     |     0.0565613 |    0.0526624 |   1.07404  | 0.282807     | -0.0466551 |  0.159778  |\n| region_Southwest |     0.0505761 |    0.0471982 |   1.07157  | 0.283914     | -0.0419307 |  0.143083  |\n\n- **Log-Likelihood**: -3258.1  \n- **Pseudo R¬≤ (Cox & Snell)**: 0.093  \n- **Number of Observations**: 1500  \n- **Model Family**: Poisson (log link)\n\n---\n\n### Estimated Effect of Blueprinty's Software\n\n- **Average Treatment Effect (Predicted)**: **0.7928 patents**\n\n\n\ntodo: Interpret the results. The Poisson regression results (as shown in the summary table from the statsmodels.GLM() output) indicate that being a Blueprinty customer is significantly associated with an increase in the number of patents awarded. Specifically, the coefficient for the iscustomer variable is 0.208 with a standard error of 0.031 and a highly significant z-score of 6.72 (p &lt; 0.001). This implies that, holding all else constant, being a Blueprinty customer increases the expected number of patents by approximately 23% (since exp(0.208)‚âà1.231. This estimated effect is further validated by the counterfactual prediction using the fitted model. When we simulate predicted patent counts for all firms assuming they are customers versus are not customers, the average treatment effect is 0.793 patents.\nThat is, on average, firms would receive nearly one additional patent over 5 years if they adopted Blueprinty‚Äôs software. Age also plays a substantial role. The age variable has a coefficient of 1.076, and age_squared has a coefficient of -1.182, both highly significant. These results (from the manually coded likelihood model and confirmed in glm_results) suggest a nonlinear relationship: as firms age, their patenting tends to increase, but the negative coefficient on age squared shows diminishing returns ‚Äî very old firms see smaller increases or even a decrease in patents. Regarding regional controls, none of the regional dummy variables (e.g., region_Northeast, region_South) are statistically significant, with p-values well above 0.1. This suggests that regional location does not significantly influence patent success once age and customer status are accounted for. The model‚Äôs overall fit is reasonable for count data, with a log-likelihood of -3258.1 and a pseudo R¬≤ of 0.093, indicating that while most of the variation in patent counts remains unexplained (common in Poisson models), the model still captures meaningful effects, especially related to customer status and firm age.\nTo understand the real-world effect of Blueprinty‚Äôs software on patent success, we created a counterfactual simulation using our fitted Poisson regression model. Since the beta coefficient for the iscustomer variable (0.208) is not directly interpretable in terms of raw patent counts, we built two hypothetical scenarios to make the impact easier to understand. In the first scenario (X_0), we assumed that none of the firms were Blueprinty customers. In the second scenario (X_1), we assumed that all firms were customers. Everything else about the firms (age, region, etc.) stayed exactly the same in both scenarios. We then used the model to predict the expected number of patents for each firm under both cases. By subtracting the predicted patent counts for the non-customer scenario from the customer scenario, and then averaging the difference across all firms, we found that being a Blueprinty customer is predicted to result in about 0.793 more patents over a 5-year period. This is a meaningful difference, especially when the average firm in the dataset is awarded around 3.68 patents (as shown earlier in the summary statistics). In short, this simulation confirms that Blueprinty‚Äôs software is associated with a moderate but practically significant increase in patenting success. It‚Äôs not just a statistically significant relationship, it‚Äôs one that likely has real impact for the firms using the software."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln \nimport statsmodels.api as sm\n\nairbnb_df = pd.read_csv('airbnb.csv')\nblueprinty_df = pd.read_csv('blueprinty.csv')\n\nFirms using Blueprinty‚Äôs software (customers) typically have more patents on average (around 4.13 patents) compared to non-customers (about 3.47 patents). The histograms reveal that both groups have distributions skewed toward lower numbers of patents, but Blueprinty customers generally have a higher frequency of firms with a larger number of patents. This preliminary analysis suggests a positive association between using Blueprinty‚Äôs software and obtaining patents.\n\nplt.figure(figsize=(10, 5))\nblueprinty_df[blueprinty_df['iscustomer'] == 0]['patents'].hist(alpha=0.5, label='Non-customer', bins=20)\nblueprinty_df[blueprinty_df['iscustomer'] == 1]['patents'].hist(alpha=0.5, label='Customer', bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\nmean_patents = blueprinty_df.groupby('iscustomer')['patents'].mean()\nprint(mean_patents)\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nCustomers and non-customers differ in regional representation: customers are predominantly located in the Northeast, whereas non-customers are more evenly spread across various regions. Regarding age, there‚Äôs little practical difference between the two groups, although customers tend to be slightly older on average (approximately 26.9 years) compared to non-customers (approximately 26.1 years). This indicates regional factors may play a role in the decision to adopt Blueprinty‚Äôs software, while age differences are relatively minor.\n\nregion_counts = pd.crosstab(blueprinty_df['region'], blueprinty_df['iscustomer'])\nregion_counts.plot(kind='bar', figsize=(10,5))\nplt.title(\"Number of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count of Firms\")\nplt.legend(title='Customer Status', labels=['Non-customer', 'Customer'])\nplt.show()\n\nblueprinty_df.boxplot(column='age', by='iscustomer', figsize=(8,5))\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Customer Status (0 = Non-customer, 1 = Customer)\")\nplt.ylabel(\"Firm Age\")\nplt.suptitle('')\nplt.show()\n\nprint(blueprinty_df.groupby('iscustomer')['age'].mean())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the probability density function is:\n¬†\\(f(|\\lambda) = \\frac{e^{-\\lambda}\\lambda^{Y}}{Y!}\\)\n\nThus, the likelihood function for the observed data \\((Y_1, Y_2, \\dots, Y_n)\\) is: ¬†\\(L(\\lambda|Y) = \\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\lambda^{Y_i}}{Y_i!}\\)\n\nAnd the corresponding log-likelihood function is: ¬†\\(\\ell(\\lambda|Y) = \\sum_{i=1}^{n}\\left[Y_i\\log(\\lambda) - \\lambda - \\log(Y_i!)\\right]\\)\n\n\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Calculate the Poisson log-likelihood for given lambda and observed Y.\n    \n    Parameters:\n    lam (float): Poisson parameter lambda (&gt; 0)\n    Y (array-like): Observed counts\n    \n    Returns:\n    float: Log-likelihood value\n    \"\"\"\n    if lam &lt;= 0:\n        return -np.inf\n    \n    log_factorials = np.array([np.sum(np.log(np.arange(1, y+1))) if y &gt; 0 else 0 for y in Y])\n    log_likelihood = np.sum(Y * np.log(lam) - lam - log_factorials)\n    \n    return log_likelihood\n\n\nY = blueprinty_df['patents'].values\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nloglike_values = [poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, loglike_values, label='Log-Likelihood')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood across Lambda Values')\nplt.axvline(lambda_values[np.argmax(loglike_values)], color='r', linestyle='--', label='Max Log-Likelihood')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStarting from the log-likelihood function:\n¬†\\(\\ell(\\lambda|Y) = \\sum_{i=1}^{n}\\left[Y_i\\log(\\lambda) - \\lambda - \\log(Y_i!)\\right]\\)\n\nTake the first derivative with respect to \\((\\lambda\\)\\):\n¬†\\(\\frac{d\\ell(\\lambda|Y)}{d\\lambda} = \\sum_{i=1}^{n}\\left(\\frac{Y_i}{\\lambda} - 1\\right)\\)\n\nSet this derivative equal to zero to find the maximum likelihood estimate (MLE):\n¬†\\(\\sum_{i=1}^{n}\\left(\\frac{Y_i}{\\lambda} - 1\\right) = 0\\)\n\nSolve for \\((\\lambda\\)\\):\n¬†\\(\\frac{\\sum_{i=1}^{n} Y_i}{\\lambda} - n = 0 \\quad \\Longrightarrow \\quad \\lambda = \\frac{\\sum_{i=1}^{n} Y_i}{n} = \\bar{Y}\\)\n\nThus, the maximum likelihood estimate for \\((\\lambda)\\) is simply the sample mean, \\((\\bar{Y})\\), which intuitively makes sense since the mean of a Poisson distribution is \\((\\lambda)\\).\n\nY = blueprinty_df['patents'].values\n\ndef neg_poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lam) - lam)\n\ninitial_guess = np.mean(Y)\n\nresult = minimize(neg_poisson_loglikelihood, \n                  x0=[initial_guess], \n                  args=(Y,),\n                  bounds=[(1e-5, None)])\n\nlambda_mle = result.x[0]\nprint(\"MLE for lambda:\", lambda_mle)\n\nMLE for lambda: 3.6846666666676033\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X_beta = X @ beta\n    X_beta = np.clip(X_beta, -20, 20)  # prevent overflow in exp()\n    lambda_i = np.exp(X_beta)\n    log_factorials = gammaln(Y + 1)  # stable log-factorial\n    loglik = np.sum(Y * X_beta - lambda_i - log_factorials)\n    return -loglik\n\n\nblueprinty_df['age_squared'] = blueprinty_df['age'] ** 2\nscaler = StandardScaler()\nblueprinty_df[['age', 'age_squared']] = scaler.fit_transform(blueprinty_df[['age', 'age_squared']])\n\nX = pd.get_dummies(blueprinty_df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX.insert(0, 'intercept', 1)\nX_matrix = X.to_numpy(dtype=np.float64)\nY_vector = blueprinty_df['patents'].to_numpy(dtype=np.int32)\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta)\n    X_beta = X @ beta\n    X_beta = np.clip(X_beta, -20, 20)  \n    lambda_i = np.exp(X_beta)\n    log_factorials = gammaln(Y + 1)  \n    loglik = np.sum(Y * X_beta - lambda_i - log_factorials)\n    return -loglik \n\nbeta_start = np.zeros(X_matrix.shape[1])\nregression_result = minimize(\n    poisson_regression_loglikelihood,\n    beta_start,\n    args=(Y_vector, X_matrix),\n    method='BFGS'\n)\n\nbeta_hat = regression_result.x\nhessian_inv = regression_result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\ncoef_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n                  Coefficient  Std. Error\nintercept            1.188966    0.038775\nage                  1.076022    0.115656\nage_squared         -1.181549    0.189616\niscustomer           0.207591    0.031799\nregion_Northeast     0.029170    0.064828\nregion_Northwest    -0.017574    0.070656\nregion_South         0.056561    0.058948\nregion_Southwest     0.050576    0.067469\n\n\n\nX_clean = X.drop(columns='intercept', errors='ignore').astype(float)\nX_glm = sm.add_constant(X_clean)\nY_numeric = Y_vector.astype(int)\n\nglm_model = sm.GLM(Y_numeric, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\ncustomer_col = [col for col in glm_results.model.exog_names if 'iscustomer' in col][0]\n\nX1_glm = X_glm.copy()\nX0_glm = X_glm.copy()\nX1_glm[customer_col] = 1\nX0_glm[customer_col] = 0\n\nX1_glm = X1_glm[glm_results.model.exog_names]\nX0_glm = X0_glm[glm_results.model.exog_names]\n\ny_pred_1 = glm_results.predict(X1_glm)\ny_pred_0 = glm_results.predict(X0_glm)\naverage_treatment_effect = np.mean(y_pred_1 - y_pred_0)\n\nfrom tabulate import tabulate\n\ncoef_df = glm_results.summary2().tables[1].reset_index()\ncoef_df.columns = ['Variable', 'Coefficient', 'Std. Error', 'z', 'P&gt;|z|', 'CI Lower', 'CI Upper']\nsummary_table = tabulate(coef_df, headers=\"keys\", tablefmt=\"github\", showindex=False)\n\nlog_likelihood = glm_results.llf\npseudo_r2 = 1 - glm_results.deviance / glm_results.null_deviance\nn_obs = glm_results.nobs\n\nfinal_summary = f\"\"\"\n###  Poisson Regression Results (`statsmodels.GLM`)\n\n{summary_table}\n\n- **Log-Likelihood**: {log_likelihood:.1f}  \n- **Pseudo R¬≤ (Cox & Snell)**: {pseudo_r2:.3f}  \n- **Number of Observations**: {int(n_obs)}  \n- **Model Family**: Poisson (log link)\n\n---\n\n### Estimated Effect of Blueprinty's Software\n\n- **Average Treatment Effect (Predicted)**: **{average_treatment_effect:.4f} patents**\n\"\"\"\nprint(final_summary)\n\n\n###  Poisson Regression Results (`statsmodels.GLM`)\n\n| Variable         |   Coefficient |   Std. Error |          z |        P&gt;|z| |   CI Lower |   CI Upper |\n|------------------|---------------|--------------|------------|--------------|------------|------------|\n| const            |     1.18897   |    0.0367361 |  32.3651   | 8.51588e-230 |  1.11696   |  1.26097   |\n| age              |     1.07602   |    0.10041   |  10.7162   | 8.5396e-27   |  0.879221  |  1.27282   |\n| age_squared      |    -1.18155   |    0.102625  | -11.5132   | 1.1315e-30   | -1.38269   | -0.980407  |\n| iscustomer       |     0.207591  |    0.0308953 |   6.71918  | 1.82751e-11  |  0.147037  |  0.268144  |\n| region_Northeast |     0.0291701 |    0.0436255 |   0.668647 | 0.50372      | -0.0563343 |  0.114674  |\n| region_Northwest |    -0.0175745 |    0.0537806 |  -0.326782 | 0.743833     | -0.122983  |  0.0878335 |\n| region_South     |     0.0565613 |    0.0526624 |   1.07404  | 0.282807     | -0.0466551 |  0.159778  |\n| region_Southwest |     0.0505761 |    0.0471982 |   1.07157  | 0.283914     | -0.0419307 |  0.143083  |\n\n- **Log-Likelihood**: -3258.1  \n- **Pseudo R¬≤ (Cox & Snell)**: 0.093  \n- **Number of Observations**: 1500  \n- **Model Family**: Poisson (log link)\n\n---\n\n### Estimated Effect of Blueprinty's Software\n\n- **Average Treatment Effect (Predicted)**: **0.7928 patents**\n\n\n\ntodo: Interpret the results. The Poisson regression results (as shown in the summary table from the statsmodels.GLM() output) indicate that being a Blueprinty customer is significantly associated with an increase in the number of patents awarded. Specifically, the coefficient for the iscustomer variable is 0.208 with a standard error of 0.031 and a highly significant z-score of 6.72 (p &lt; 0.001). This implies that, holding all else constant, being a Blueprinty customer increases the expected number of patents by approximately 23% (since exp(0.208)‚âà1.231. This estimated effect is further validated by the counterfactual prediction using the fitted model. When we simulate predicted patent counts for all firms assuming they are customers versus are not customers, the average treatment effect is 0.793 patents.\nThat is, on average, firms would receive nearly one additional patent over 5 years if they adopted Blueprinty‚Äôs software. Age also plays a substantial role. The age variable has a coefficient of 1.076, and age_squared has a coefficient of -1.182, both highly significant. These results (from the manually coded likelihood model and confirmed in glm_results) suggest a nonlinear relationship: as firms age, their patenting tends to increase, but the negative coefficient on age squared shows diminishing returns ‚Äî very old firms see smaller increases or even a decrease in patents. Regarding regional controls, none of the regional dummy variables (e.g., region_Northeast, region_South) are statistically significant, with p-values well above 0.1. This suggests that regional location does not significantly influence patent success once age and customer status are accounted for. The model‚Äôs overall fit is reasonable for count data, with a log-likelihood of -3258.1 and a pseudo R¬≤ of 0.093, indicating that while most of the variation in patent counts remains unexplained (common in Poisson models), the model still captures meaningful effects, especially related to customer status and firm age.\nTo understand the real-world effect of Blueprinty‚Äôs software on patent success, we created a counterfactual simulation using our fitted Poisson regression model. Since the beta coefficient for the iscustomer variable (0.208) is not directly interpretable in terms of raw patent counts, we built two hypothetical scenarios to make the impact easier to understand. In the first scenario (X_0), we assumed that none of the firms were Blueprinty customers. In the second scenario (X_1), we assumed that all firms were customers. Everything else about the firms (age, region, etc.) stayed exactly the same in both scenarios. We then used the model to predict the expected number of patents for each firm under both cases. By subtracting the predicted patent counts for the non-customer scenario from the customer scenario, and then averaging the difference across all firms, we found that being a Blueprinty customer is predicted to result in about 0.793 more patents over a 5-year period. This is a meaningful difference, especially when the average firm in the dataset is awarded around 3.68 patents (as shown earlier in the summary statistics). In short, this simulation confirms that Blueprinty‚Äôs software is associated with a moderate but practically significant increase in patenting success. It‚Äôs not just a statistically significant relationship, it‚Äôs one that likely has real impact for the firms using the software."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "href": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nairbnb_missing = airbnb_df.isna().sum()\n\nrelevant_cols = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms',\n    'price', 'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\nairbnb_clean = airbnb_df[relevant_cols].dropna()\n\nairbnb_clean = pd.get_dummies(airbnb_clean, columns=['room_type', 'instant_bookable'], drop_first=True)\n\nY_airbnb = airbnb_clean['number_of_reviews']\nX_airbnb = airbnb_clean.drop(columns='number_of_reviews')\n\nX_airbnb = X_airbnb.astype(float)\nY_airbnb = Y_airbnb.astype(int)\n\nX_airbnb = sm.add_constant(X_airbnb)\n\nairbnb_model = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())\nairbnb_results = airbnb_model.fit()\n\nairbnb_summary = airbnb_results.summary2().tables[1]\nairbnb_summary\n\n#Plots\nplt.style.use('ggplot')\n\n# Plot distribution of number_of_reviews\nplt.figure(figsize=(8, 5))\nsns.histplot(airbnb_clean['number_of_reviews'], bins=50, kde=False)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\nairbnb_clean['room_type_Private room'] = airbnb_clean['room_type_Private room'].astype(int)\n\n# Boxplot of reviews by room type\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='room_type_Private room', y='number_of_reviews', data=airbnb_clean)\nplt.xticks([0, 1], ['Entire Home/Shared', 'Private Room'])\nplt.title('Number of Reviews by Room Type')\nplt.xlabel('Room Type')\nplt.ylabel('Number of Reviews')\nplt.tight_layout()\nplt.show()\n\n# Scatterplot of reviews vs. price\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='price', y='number_of_reviews', data=airbnb_clean, alpha=0.3)\nplt.title('Number of Reviews vs. Price')\nplt.xlabel('Price ($)')\nplt.ylabel('Number of Reviews')\nplt.tight_layout()\nplt.show()\n\n# Correlation heatmap for numeric predictors\nplt.figure(figsize=(10, 8))\nsns.heatmap(airbnb_clean.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\nplt.title('Correlation Heatmap of Variables')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand what drives the number of reviews (used as a proxy for bookings), we ran a Poisson regression on several listing features in the Airbnb dataset. The results provide clear insights into what influences guest engagement and booking behavior. The strongest and most intuitive finding is that the longer a listing has been active (days), the more reviews it has. The coefficient is small (0.000051), but it‚Äôs applied to values that can be in the thousands, meaning its real-world impact is substantial. This simply reflects that more time online gives a listing more chances to be booked and reviewed.\nThe ‚Äúinstant bookable‚Äù feature stands out as particularly important. Listings that can be booked instantly ‚Äî without host approval ‚Äî have a coefficient of 0.346, which means they are predicted to get about 41% more reviews than those that require approval exp(0.346)‚âà1.41). This is a large and meaningful effect, suggesting that hosts should enable this feature if they want to increase visibility and bookings. Cleanliness ratings also matter a lot. Each one-point increase in the cleanliness score is associated with a 12% increase in the number of reviews (coef = 0.113). This makes sense: guests reward clean listings with more bookings and reviews. However, two other review scores ‚Äî location and value ‚Äî surprisingly have negative coefficients (-0.077 and -0.091, respectively).\nThis could mean that while those scores matter to guests‚Äô satisfaction, they don‚Äôt directly drive higher booking rates, or it could reflect some counterintuitive relationship (like higher-rated locations needing fewer reviews to stay popular).\nLooking at room types, shared rooms receive significantly fewer reviews than entire homes (coef = -0.246), which aligns with common user preferences ‚Äî people usually prefer privacy. Private rooms also see slightly fewer reviews compared to entire homes, but the effect is much smaller (-0.011). The model also finds a slight negative effect of price (-0.000018), indicating that higher-priced listings tend to receive marginally fewer reviews, likely because they are less frequently booked. Similarly, more bathrooms are associated with fewer reviews (-0.118), which might seem strange, but could reflect that listings with multiple bathrooms tend to be larger or more expensive properties booked less often."
  }
]